
Quick intro
脳の類推に訴えることなくニューラルネットワークを導入することは可能。

線形分類のセクションでは、式s=Wxを使用して、
画像に与えられた異なる視覚カテゴリのスコアを計算した。

例示的なニューラルネットワークは、代わりにs = W2max（0、W1x）を計算する。 


W1は、例えば、画像を100次元の中間ベクトルに変換する行列であってもよい。 

関数max（0、 - ）は、要素ごとに適用される非線形性。

非線形性については、いくつかの選択肢があるが（以下で検討する）、
これは一般的な選択であり、ゼロからゼロまでのすべてのアクティベーションを単にしきい値する。

行列W2によって、クラススコアとして解釈する数値を再度取得する。

非線形性は計算上重要である。

 パラメータW2、W1は確率勾配降下で学習され、それらの勾配は逆伝播で計算される



同様に、3層ニューラルネットワークは、s3 = W3max（0、W2max（0、W1x））と書ける。
ここで、W3、W2、W1はすべて学習すべきパラメータである。
中間の隠れベクトルのサイズはネットワークのハイパーパラメータ。


これらの計算をニューロン/ネットワークの観点から解釈。



Modeling one neuron
1つのニューロンのモデリング

ニューラルネットワークの領域
    もともとは主に生物学的な神経系のモデリングが目標であった。
    発展し、機械学習のタスクで良い結果を達成した。

    ニューロン: 脳の基本的な計算単位
    約860億個のニューロンが人間の神経系に見られ、
    それらは約 10^14 〜 10^15個のシナプスと関連している。

    樹状突起(dendrites)から入力を得て、単一の軸索(axon)に沿って出力する。
    軸索は最終的に分岐し、シナプスを介して他のニューロンの樹状突起につながる。

    ニューロンからの入力はシナプスの重みと掛け合わされて、次のニューロンに渡される。

    シナプスの強さ（重みw）が学習可能であり、
    あるニューロンの別のニューロンの影響力を制御するという考え方である。
    （およびその方向：興奮性（正の重み）または抑制性（負の重み））
    
    ニューロンは樹状突起からの入力を累積し、その和がしきい値を超えると発火する。
    軸索に沿ってspikeを送る。

    重要なのはスパイクの正しいタイミングではなく、発火の頻度のみが情報を伝達すると仮定する。
    軸索に沿ったスパイクの頻度を表す活性化関数fの発火速度をモデル化する。

    歴史的に一般的な活性化関数は シグモイド関数
        実数値入力を0から1の間の範囲に収縮させるため。
    
言い換えれば、各ニューロンは、入力とその重みでドット積を実行し、
バイアスを加え、非線形性（または活性化関数）を適用する。


Coarse model
粗いモデル
    本来はスパイクのタイミングが非常に重要であるが、
    大変なので、非常に粗いモデルを想定する。


Single neuron as a linear classifier
線形分類器としての単一ニューロン


Binary Softmax classifier.
バイナリSoftmax分類器
σ（Σiwixi+ b）をクラスP（yi = 1 | xi; w）のうちの1つの確率と解釈することができる。
全クラスの存在確率の合計は1になるため、
他のクラスの確率はP（yi = 0 | xi; w）= 1-P（yi = 1 | xi; w）となる。
クロスエントロピー損失を定式化し、それを最適化することでバイナリSoftmax分類器につながる。
（ロジスティック回帰とも呼ばれる）

Binary SVM classifier
バイナリSVM分類器
ニューロンの出力に最大マージンヒンジ損失を付けて、
バイナリサポートベクトルマシンになるようにトレーニングすることができる。


Regularization interpretation
正規化の解釈
SVM/Softmaxの両方の正則化損失は、この生物学的見解では、
各パラメータの更新後にすべてのシナプス重みwを0に近づける効果を有するため、
"徐々に忘れる"と解釈される可能性がある。



Commonly used activation functions
一般に使用されるアクティベーション機能

シグモイドのはニューロンの発火の理解をしやすいため、よく用いられる。
しかし、シグモイドの非線形性は最近好まれなくなり、使用されることはほとんどない。
シグモイドには2つの大きな欠点がある。

・  シグモイドは飽和してグラデーションを殺します。シグモイドニューロンの非常に望ましくない特性は、ニューロンの活性化が0または1のいずれかのテイルで飽和すると、これらの領域における勾配がほぼゼロであることである。バックプロパゲーションの間、この（ローカル）勾配は、全目的のためにこのゲートの出力の勾配に乗算されることを思い出してください。したがって、局所的な勾配が非常に小さい場合、勾配を効果的に「殺す」ことになり、ニューロンからその重みまで、そして再帰的にそのデータに流れる信号はほとんどありません。さらに、飽和を防ぐためにシグモイドニューロンの重みを初期化するときには、慎重を期す必要があります。たとえば、最初の重みが大きすぎると、ほとんどのニューロンは飽和し、ネットワークはほとんど学習されません。

・  x>0f=wTx+bwf）。これは、重みの勾配更新において望ましくないジグザグのダイナミクスを導入する可能性がある。しかし、これらの勾配が一括してデータに追加されると、最終的な更新にはさまざまな兆候があり、この問題は多少緩和されます。したがって、これは不都合ですが、上記の飽和活性化問題と比較して重大な結果はより少なくなります。

