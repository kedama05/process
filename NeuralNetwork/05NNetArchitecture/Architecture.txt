
Quick intro
脳の類推に訴えることなくニューラルネットワークを導入することは可能。

線形分類のセクションでは、式s=Wxを使用して、
画像に与えられた異なる視覚カテゴリのスコアを計算した。

例示的なニューラルネットワークは、代わりにs = W2max（0、W1x）を計算する。 


W1は、例えば、画像を100次元の中間ベクトルに変換する行列であってもよい。 

関数max（0、 - ）は、要素ごとに適用される非線形性。

非線形性については、いくつかの選択肢があるが（以下で検討する）、
これは一般的な選択であり、ゼロからゼロまでのすべてのアクティベーションを単にしきい値する。

行列W2によって、クラススコアとして解釈する数値を再度取得する。

非線形性は計算上重要である。

 パラメータW2、W1は確率勾配降下で学習され、それらの勾配は逆伝播で計算される



同様に、3層ニューラルネットワークは、s3 = W3max（0、W2max（0、W1x））と書ける。
ここで、W3、W2、W1はすべて学習すべきパラメータである。
中間の隠れベクトルのサイズはネットワークのハイパーパラメータ。


これらの計算をニューロン/ネットワークの観点から解釈。



Modeling one neuron
1つのニューロンのモデリング

ニューラルネットワークの領域
    もともとは主に生物学的な神経系のモデリングが目標であった。
    発展し、機械学習のタスクで良い結果を達成した。

    ニューロン: 脳の基本的な計算単位
    約860億個のニューロンが人間の神経系に見られ、
    それらは約 10^14 〜 10^15個のシナプスと関連している。

    樹状突起(dendrites)から入力を得て、単一の軸索(axon)に沿って出力する。
    軸索は最終的に分岐し、シナプスを介して他のニューロンの樹状突起につながる。

    ニューロンからの入力はシナプスの重みと掛け合わされて、次のニューロンに渡される。

    シナプスの強さ（重みw）が学習可能であり、
    あるニューロンの別のニューロンの影響力を制御するという考え方である。
    （およびその方向：興奮性（正の重み）または抑制性（負の重み））
    
    ニューロンは樹状突起からの入力を累積し、その和がしきい値を超えると発火する。
    軸索に沿ってspikeを送る。

    重要なのはスパイクの正しいタイミングではなく、発火の頻度のみが情報を伝達すると仮定する。
    軸索に沿ったスパイクの頻度を表す活性化関数fの発火速度をモデル化する。

    歴史的に一般的な活性化関数は シグモイド関数
        実数値入力を0から1の間の範囲に収縮させるため。
    
言い換えれば、各ニューロンは、入力とその重みでドット積を実行し、
バイアスを加え、非線形性（または活性化関数）を適用する。


Coarse model
粗いモデル
    本来はスパイクのタイミングが非常に重要であるが、
    大変なので、非常に粗いモデルを想定する。


Single neuron as a linear classifier
線形分類器としての単一ニューロン


Binary Softmax classifier.
バイナリSoftmax分類器
σ（Σiwixi+ b）をクラスP（yi = 1 | xi; w）のうちの1つの確率と解釈することができる。
全クラスの存在確率の合計は1になるため、
他のクラスの確率はP（yi = 0 | xi; w）= 1-P（yi = 1 | xi; w）となる。
クロスエントロピー損失を定式化し、それを最適化することでバイナリSoftmax分類器につながる。
（ロジスティック回帰とも呼ばれる）

Binary SVM classifier
バイナリSVM分類器
ニューロンの出力に最大マージンヒンジ損失を付けて、
バイナリサポートベクトルマシンになるようにトレーニングすることができる。


Regularization interpretation
正規化の解釈
SVM/Softmaxの両方の正則化損失は、この生物学的見解では、
各パラメータの更新後にすべてのシナプス重みwを0に近づける効果を有するため、
"徐々に忘れる"と解釈される可能性がある。



Commonly used activation functions
一般に使用されるアクティベーション機能

シグモイドのはニューロンの発火の理解をしやすいため、よく用いられる。
しかし、シグモイドの非線形性は最近好まれなくなり、使用されることはほとんどない。
シグモイドには2つの大きな欠点がある。

1.  シグモイドは飽和して勾配を殺す。
    
    シグモイドニューロンの非常に望ましくない特性は、
    ニューロンの活性化が0または1のいずれかのテイルで飽和すると、
    これらの領域における勾配がほぼゼロであることである。
    
    バックプロパゲーションの間、この（ローカル）勾配は、
    全目的のためにこのゲートの出力の勾配に乗算されることを思い出してください。
    
    したがって、局所的な勾配が非常に小さい場合、勾配を効果的に「殺す」ことになり、
    ニューロンからその重みまで、そして再帰的にそのデータに流れる信号はほとんどない。
    
    さらに、飽和を防ぐためにシグモイドニューロンの重みを初期化するときには、慎重を期す必要がある。
 
    たとえば、最初の重みが大きすぎると、ほとんどのニューロンは飽和し、ネットワークはほとんど学習されない。


2.  シグモイドの出力はゼロ中心ではない。
        ニューラルネットワークにおける後の処理層のニューロンがゼロ中心ではないデータを受信するため、望ましくない。
            ニューロンに入るデータが常に正である場合、勾配降下中のダイナミクスに影響を及ぼす。
            （例えば、f> wTx + bにおいてx> 0の要素単位で）
                →バックプロパゲーション中の重みwの勾配は、すべて正または負のいずれかになる。（式f全体の勾配に依存する）
        重みの勾配更新において望ましくないジグザグの変動を招く可能性がある。

        しかし、これらの勾配が一括してデータに追加されると、
        最終的な更新は更新は可変兆候を持つことがあり、この問題は多少緩和される。

        したがって、不都合ではあるが、
        上記の飽和活性化問題と比較して重大な結果はより少なくなる。



Tanh
    実数の数値を[-1,1]の範囲にスカッシュする。
    シグモイドニューロンと同様に、その活性化は飽和するが、
    シグモイドニューロンとは異なり、その出力はゼロ中心である。

    Tanhの非線形性は常にシグモイド非線形性よりも好ましい。

また、tanhニューロンは、単純に縮尺変更されたシグモイドニューロンであり、
特に以下が成り立つことに留意されたい: tanh（x）=2σ（2x）-1。



ReLU
    整流された線形ユニットはここ数年で非常に普及してる。

    ReLUでは、関数f（x）= max（0、x）を計算する。
    つまり、活性化は単にゼロで閾値設定される。

ReLUsの使用にはいくつかの賛否両論がある。
        
・  （+）シグモイド/タン関数と比較して確率的勾配降下の収束を大幅に加速することが分かった。
    これは、その線形で飽和しない形式によるものであると主張されている。

・  （+）高価な操作（指数関数など）を伴うtanh / sigmoidニューロンと比較して、
    ReLUは単にゼロの活性化のマトリックスを閾値処理することで実装できる。

・  （-）残念ながら、ReLUユニットは訓練中に壊れやすく、「死ぬ」可能性がある。
    例えば、ReLUニューロンを流れる大きな勾配は、
    ニューロンがデータポイント上で決して活性化しないように重みを更新させる可能性がある。
    これが起こると、ユニットを流れる勾配は、その点から永遠にゼロになる。
    
    つまり、ReLUユニットは、データマニホールドから叩き出される可能性があるため、
    トレーニング中に不可逆的に死ぬことがある。
    
    たとえば、学習率が高すぎる場合、ネットワークの40％が「死んでいる」になることがある。
    （つまり、トレーニングデータセット全体でアクティブにならないニューロン）
    学習率を適切に設定することで、これはあまり問題にならない。




Leaky ReLU

Leaky ReLUは、「死んでいるReLU」問題を修正しようとする試みである。
x <0のときに関数がゼロになる代わりに、漏れたReLUは、代わりに小さな負の傾き（0.01程度）を持つ。
すなわち、関数は、f（x）= 1（x <0）（αx）+1（x> = 0）（x）を計算する。ここで、αは小さな定数。
一部の人々は、この形式のアクティベーション機能で成功を報告するが、結果は必ずしも一致しない。
負の領域における勾配は、各ニューロンのパラメータにすることもできる。
しかし、タスク間の利益の一貫性は現在不明である。



Maxout
非線形性が重みとデータとの間の内積に適用される関数形式f（wTx + b）を有さない他のタイプのユニットが提案されている。
Maxoutニューロン: ReLUとそのリーキーバージョンを一般化する
                関数max（wT1x + b1、wT2x + b2）を計算する。 

ReLUとLeaky ReLUの両方がこのフォームの特別なケースであることに注意してください。
（たとえば、ReLUにはw1、b1 = 0があります）
→   Maxoutニューロンは、ReLUユニットのすべての利点を享受し、その欠点を持たない。
        ReLUユニットの利点: 動作の線形制御、飽和なし
        ReLUの欠点: 死んでいるReLU

    ReLUニューロンとは異なり、
        単一のニューロンごとにパラメータの数が倍増し、高い総数のパラメータにつながる。

一般的なタイプのニューロンとそれらの活性化機能についてのまとめ。
同じネットワークにさまざまなタイプのニューロンを混在させて一致させるのは、根本的な問題はないが、非常にまれ。



TLDR
「どのニューロンタイプを使用すればよいですか？」
ReLU非線形性を使用し、学習率に注意し、
ネットワーク内の「死んだ」ユニットの割合を監視することができる。

これが懸念される場合は、Leaky ReLUまたはMaxoutに試してみてください。
シグモイドは使用しないでください。

tanhを試してみてくださいが、ReLU / Maxoutよりも悪いと思われる。


Neural Network architectures
ニューラルネットワークアーキテクチャ

Layer-wise organization
層別組織

Neural Networks as neurons in graphs
グラフのニューロンとしてのニューラルネットワーク

ニューラルネットワークは、非周期グラフで接続されたニューロンの集合としてモデル化される。
→   いくつかのニューロンの出力は、他のニューロンへの入力となり得る。

通常のニューラルネットワークの場合、最も一般的なレイヤタイプは、
2つの隣接レイヤ間のニューロンが完全にペアワイズ接続されている完全接続レイヤですが、
単一レイヤ内のニューロンは接続を共有しません。

完全に接続されたレイヤのスタックを使用する2つのニューラルネットワークトポロジを示す。
    レイヤー内ではなく、レイヤー間でニューロン間の接続（シナプス）が存在する。


命名規則
N層ニューラルネットワークとは、入力層を数えない。

単層ニューラルネットワーク: 隠れ層のないネットワーク（出力に直接マッピングされた入力）

ロジスティック回帰やSVMは、単層のニューラルネットワークの特殊なケース。


出力レイヤー
出力レイヤーのニューロンは一般的に活性化機能を持たない。


ニューラルネットワークのサイジング
人々がニューラルネットワークのサイズを測定するためによく使用する2つの測定基準:
    ニューロンの数、またはより一般的にはパラメータの数


Example feed-forward computation
フィードフォワード計算の例

Repeated matrix multiplications interwoven with activation function
活性化関数と織り交ぜた繰り返し行列乗算。

ニューラルネットワークが階層化されている主な理由の1つ:
マトリックスベクトル演算を使用してニューラルネットワークを非常に簡単かつ効率的に評価できる。

レイヤーのすべての接続強度は、単一のマトリックスに格納することができる。

ここで、すべての単一ニューロンは、W1の行にその重みを有するため、
行列ベクトル乗算np.dot（W1、x）は、その層内のすべてのニューロンの活性化を評価する。

この3層ニューラルネットワークの全順方向通過は、活性化関数の適用と織り込まれた単純に3つの行列乗算である。


上記のコードでは、W1、W2、W3、b1、b2、b3はネットワークの学習可能なパラメータです。

また、単一の入力列ベクトルを持つ代わりに、変数xはトレーニングデータのバッチ全体を保持することができる。
（各入力例はxの列になります）
次に、すべての例が並列で効率的に評価される。

最終的なニューラルネットワーク層は通常、活性化機能を有さないことに留意されたい。
（例えば、分類設定において（実数値）クラススコアを表す）



Representational power
代表権

完全に接続されたレイヤーを持つニューラルネットワークを見る1つの方法は、
ネットワークの重みによってパラメータ化された一連の関数を定義することです。
発生する自然な問題は次のとおりです。この関数ファミリの表現力は何ですか？
特に、ニューラルネットワークでモデル化できない関数はありますか？


少なくとも1つの隠れ層を有するニューラルネットワークは普遍的近似器であることが判明している。

すなわち、任意の連続関数f（x）といくつかのε> 0が与えられれば、
∀xのような1つの隠れ層を持つニューラルネットワークg（x）が存在することが分かる。
∀x、| f（x）-g（x）| <ε

換言すれば、ニューラルネットワークは、任意の連続関数を近似することができる。


（例えば、Sigmoidal Functionの重畳による近似（1989年）（pdf）、またはMichael Nielsenによるこの直感的な説明を参照）

（合理的な非直線性の選択、例えばシグモイド）



1つの隠れたレイヤーで関数を近似することができれば、
より多くのレイヤーを使用し、より深く進むのはなぜですか？

その答えは、2層ニューラルネットワークが普遍的な近似であるという事実は、
数学的にはかわいいが、実際には比較的弱く無用な記述であるということである。

ある次元では、a、b、cがパラメータベクトルである場合、「インジケータバンプの合計」関数g（x）=Σici𝟙（ai <x <bi）であるが、これは普遍的な近似であるが、 機械学習の形式

ニューラルネットワークは、実際に遭遇するデータの統計的特性によく合った素敵でスムーズな機能をコンパクトに表現し、
最適化アルゴリズム（例：勾配降下法）を使用して簡単に学習できるため、実際にはうまく機能します。

同様に、より深いネットワーク（複数の隠れたレイヤを持つ）が単一の隠れたレイヤーのネットワークよりもうまくいくという事実は、
その表現力が等しいという事実にもかかわらず、経験的な観察である。



脇に、実際には、3層ニューラルネットワークが2層ネットより優れていることがよくありますが、
さらに深くなると（4,5,6層）、めったに役に立たないことはほとんどありません。

これは、奥行きが良好な認識システムにとって非常に重要な構成要素であることが判明している畳み込みネットワーク（例えば、10の学習可能層のオーダー）とは対照的である。

この観察の1つの議論は、画像が階層構造を含むことである（例えば、顔は目から構成され、エッジなどで構成されている）ので、
処理のいくつかの層はこのデータ領域に対して直感的である。



完全なストーリーは、もちろん、はるかに関与しており、最近の多くの研究の話題です。
これらのトピックに興味がある場合は、さらに読むことをお勧めします：


Bengio、Goodfellow、Courville、特に第6.4章のプレスでの深い学習の本。

ディープネットは本当に深くする必要がありますか？

FitNets：細いディープネットのヒント




Setting number of layers and their sizes
レイヤーの数とサイズの設定

実用的な問題に直面したときにどのようなアーキテクチャを使用するかをどのように決定するのですか？
私たちは隠れた層を使用すべきでしょうか？1つの隠れた層？2つの隠れた層？
各レイヤーの大きさはどれくらいですか？まず、ニューラルネットワークのレイヤーのサイズと数を増やすと、
ネットワークの容量が増加することに注意してください。
すなわち、表現可能な関数の空間は、ニューロンが多くの異なる関数を表現するために協力することができるために成長する。
たとえば、2つの次元でバイナリ分類の問題があったとします。
我々は、3つの別々のニューラルネットワークを訓練することができ、
各々はある大きさの隠れ層を1つ備え、次の分類子を得る。



上の図では、より多くのニューロンを持つニューラルネットワークがより複雑な機能を表現できることがわかります。
しかし、これは（より複雑なデータを分類することを学ぶことができるので）祝福でも、呪いでもあります。
（訓練データを過度にフィットさせる方が簡単です）
過剰適合は、高容量のモデルがデータ内のノイズの代わりに（仮定）基礎となる関係に適合する場合に発生します。
例えば、20個の隠れニューロンを有するモデルは、すべてのトレーニングデータに適合するが、
スペースを多くの不連続な赤と緑の決定領域に分割するという犠牲を払っている。
3つの隠れたニューロンを有するモデルは、データを広いストロークで分類する表現力しか有していない。
データを2つのブロブとしてモデル化し、緑色のクラスタ内の数点の赤い点を異常値として解釈します（ノイズ）。
実際には、これはテストセットのより良い一般化につながる可能性があります。



上記の考察に基づいて、データが過剰適合を防ぐのに十分複雑でない場合、
より小さいニューラルネットワークが好ましいと思われる。
しかし、これは間違っています
ー  後で議論するニューラルネットワークのオーバーフィッティング（L2正則化、ドロップアウト、入力ノイズなど）を防ぐ他の多くの方法があります。
実際には、ニューロンの数ではなくオーバーフィットを制御するために、これらの方法を使用する方が常に優れています。


これの背後にある微妙な理由は、小さなネットワークではグラデーション・デサントなどのローカル・メソッドでトレーニングするのが難しいことです。
ロス関数には極小点はほとんどありませんが、彼らは悪い（すなわち、損失が大きい）。
逆に、より大きいニューラルネットワークには極端に局所的な最小値が含まれますが、
これらの最小値は実際の損失の点ではるかに優れています。
ニューラルネットワークは凸ではないので、これらの特性を数学的に研究することは困難であるが、
これらの目的関数を理解するいくつかの試みがなされている
（例えば、最近の論文、Multilayer Networks。
実際には、小さなネットワークを訓練する場合、最終的な損失はかなりの分散を表示することができます
 -  場合によっては、運が良ければよい場所に収束することもありますが、場合によっては悪い極小。
一方、大規模なネットワークを訓練する場合は、さまざまなソリューションを見つけることができますが、
達成される最終的な損失のばらつきははるかに小さくなります。
言い換えれば、すべてのソリューションはほぼ同じくらい良好であり、
ランダム初期化の不運に頼る必要はありません。


繰り返すには、正則化強度は、ニューラルネットワークのオーバーフィットを制御するための好ましい方法である。
3つの異なる設定によって達成される結果を見ることができます。



正則化強度の効果：上の各ニューラルネットワークには20個の隠れニューロンがありますが、
正則化強度を変更すると最終決定領域がより正則化されてスムーズになります。
これらの例をこのConvNetsJSデモでプレイすることができます。


テイクアウェイは、あなたが過当なことを恐れているため、
より小さなネットワークを使用すべきではないということです。
代わりに、あなたの計算予算が許す限り大きなニューラルネットワークを使用し、
他の正則化技術を使用して過適合をコントロールする必要があります。










