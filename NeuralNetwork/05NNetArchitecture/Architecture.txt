
Quick intro
脳の類推に訴えることなくニューラルネットワークを導入することは可能。

線形分類のセクションでは、式s=Wxを使用して、
画像に与えられた異なる視覚カテゴリのスコアを計算した。

例示的なニューラルネットワークは、代わりにs = W2max（0、W1x）を計算する。 


W1は、例えば、画像を100次元の中間ベクトルに変換する行列であってもよい。 

関数max（0、 - ）は、要素ごとに適用される非線形性。

非線形性については、いくつかの選択肢があるが（以下で検討する）、
これは一般的な選択であり、ゼロからゼロまでのすべてのアクティベーションを単にしきい値する。

行列W2によって、クラススコアとして解釈する数値を再度取得する。

非線形性は計算上重要である。

 パラメータW2、W1は確率勾配降下で学習され、それらの勾配は逆伝播で計算される



同様に、3層ニューラルネットワークは、s3 = W3max（0、W2max（0、W1x））と書ける。
ここで、W3、W2、W1はすべて学習すべきパラメータである。
中間の隠れベクトルのサイズはネットワークのハイパーパラメータ。


これらの計算をニューロン/ネットワークの観点から解釈。



Modeling one neuron
1つのニューロンのモデリング

ニューラルネットワークの領域
    もともとは主に生物学的な神経系のモデリングが目標であった。
    発展し、機械学習のタスクで良い結果を達成した。

    ニューロン: 脳の基本的な計算単位
    約860億個のニューロンが人間の神経系に見られ、
    それらは約 10^14 〜 10^15個のシナプスと関連している。

    樹状突起(dendrites)から入力を得て、単一の軸索(axon)に沿って出力する。
    軸索は最終的に分岐し、シナプスを介して他のニューロンの樹状突起につながる。

    ニューロンからの入力はシナプスの重みと掛け合わされて、次のニューロンに渡される。

    シナプスの強さ（重みw）が学習可能であり、
    あるニューロンの別のニューロンの影響力を制御するという考え方である。
    （およびその方向：興奮性（正の重み）または抑制性（負の重み））
    
    ニューロンは樹状突起からの入力を累積し、その和がしきい値を超えると発火する。
    軸索に沿ってspikeを送る。

    重要なのはスパイクの正しいタイミングではなく、発火の頻度のみが情報を伝達すると仮定する。
    軸索に沿ったスパイクの頻度を表す活性化関数fの発火速度をモデル化する。

    歴史的に一般的な活性化関数は シグモイド関数
        実数値入力を0から1の間の範囲に収縮させるため。
    
言い換えれば、各ニューロンは、入力とその重みでドット積を実行し、
バイアスを加え、非線形性（または活性化関数）を適用する。


Coarse model
粗いモデル
    本来はスパイクのタイミングが非常に重要であるが、
    大変なので、非常に粗いモデルを想定する。


Single neuron as a linear classifier
線形分類器としての単一ニューロン


Binary Softmax classifier.
バイナリSoftmax分類器
σ（Σiwixi+ b）をクラスP（yi = 1 | xi; w）のうちの1つの確率と解釈することができる。
全クラスの存在確率の合計は1になるため、
他のクラスの確率はP（yi = 0 | xi; w）= 1-P（yi = 1 | xi; w）となる。
クロスエントロピー損失を定式化し、それを最適化することでバイナリSoftmax分類器につながる。
（ロジスティック回帰とも呼ばれる）

Binary SVM classifier
バイナリSVM分類器
ニューロンの出力に最大マージンヒンジ損失を付けて、
バイナリサポートベクトルマシンになるようにトレーニングすることができる。


Regularization interpretation
正規化の解釈
SVM/Softmaxの両方の正則化損失は、この生物学的見解では、
各パラメータの更新後にすべてのシナプス重みwを0に近づける効果を有するため、
"徐々に忘れる"と解釈される可能性がある。



Commonly used activation functions
一般に使用されるアクティベーション機能

シグモイドのはニューロンの発火の理解をしやすいため、よく用いられる。
しかし、シグモイドの非線形性は最近好まれなくなり、使用されることはほとんどない。
シグモイドには2つの大きな欠点がある。

1.  シグモイドは飽和して勾配を殺す。
    
    シグモイドニューロンの非常に望ましくない特性は、
    ニューロンの活性化が0または1のいずれかのテイルで飽和すると、
    これらの領域における勾配がほぼゼロであることである。
    
    バックプロパゲーションの間、この（ローカル）勾配は、
    全目的のためにこのゲートの出力の勾配に乗算されることを思い出してください。
    
    したがって、局所的な勾配が非常に小さい場合、勾配を効果的に「殺す」ことになり、
    ニューロンからその重みまで、そして再帰的にそのデータに流れる信号はほとんどない。
    
    さらに、飽和を防ぐためにシグモイドニューロンの重みを初期化するときには、慎重を期す必要がある。
 
    たとえば、最初の重みが大きすぎると、ほとんどのニューロンは飽和し、ネットワークはほとんど学習されない。


2.  シグモイドの出力はゼロ中心ではない。
        ニューラルネットワークにおける後の処理層のニューロンがゼロ中心ではないデータを受信するため、望ましくない。
            ニューロンに入るデータが常に正である場合、勾配降下中のダイナミクスに影響を及ぼす。
            （例えば、f> wTx + bにおいてx> 0の要素単位で）
                →バックプロパゲーション中の重みwの勾配は、すべて正または負のいずれかになる。（式f全体の勾配に依存する）
        重みの勾配更新において望ましくないジグザグの変動を招く可能性がある。

        しかし、これらの勾配が一括してデータに追加されると、
        最終的な更新は更新は可変兆候を持つことがあり、この問題は多少緩和される。

        したがって、不都合ではあるが、
        上記の飽和活性化問題と比較して重大な結果はより少なくなる。



Tanh
    実数の数値を[-1,1]の範囲にスカッシュする。
    シグモイドニューロンと同様に、その活性化は飽和するが、
    シグモイドニューロンとは異なり、その出力はゼロ中心である。

    Tanhの非線形性は常にシグモイド非線形性よりも好ましい。

また、tanhニューロンは、単純に縮尺変更されたシグモイドニューロンであり、
特に以下が成り立つことに留意されたい: tanh（x）=2σ（2x）-1。



ReLU
    整流された線形ユニットはここ数年で非常に普及してる。

    ReLUでは、関数f（x）= max（0、x）を計算する。
    つまり、活性化は単にゼロで閾値設定される。

ReLUsの使用にはいくつかの賛否両論がある。
        
・  （+）シグモイド/タン関数と比較して確率的勾配降下の収束を大幅に加速することが分かった。
    これは、その線形で飽和しない形式によるものであると主張されている。

・  （+）高価な操作（指数関数など）を伴うtanh / sigmoidニューロンと比較して、
    ReLUは単にゼロの活性化のマトリックスを閾値処理することで実装できる。

・  （-）残念ながら、ReLUユニットは訓練中に壊れやすく、「死ぬ」可能性がある。
    例えば、ReLUニューロンを流れる大きな勾配は、
    ニューロンがデータポイント上で決して活性化しないように重みを更新させる可能性がある。
    これが起こると、ユニットを流れる勾配は、その点から永遠にゼロになる。
    
    つまり、ReLUユニットは、データマニホールドから叩き出される可能性があるため、
    トレーニング中に不可逆的に死ぬことがある。
    
    たとえば、学習率が高すぎる場合、ネットワークの40％が「死んでいる」になることがある。
    （つまり、トレーニングデータセット全体でアクティブにならないニューロン）
    学習率を適切に設定することで、これはあまり問題にならない。




Leaky ReLU

Leaky ReLUは、「死んでいるReLU」問題を修正しようとする試みである。
x <0のときに関数がゼロになる代わりに、漏れたReLUは、代わりに小さな負の傾き（0.01程度）を持つ。
すなわち、関数は、f（x）= 1（x <0）（αx）+1（x> = 0）（x）を計算する。
一部の人々は、この形式のアクティベーション機能で成功を報告しますが、結果は必ずしも一致しない。
負の領域における勾配は、Kaiming Heらによる2015年の深海精錬に導入されたPReLUニューロンに見られるように、
各ニューロンのパラメータにすることもできる。
しかし、タスク間の一貫性は現在のところ 不明



Maxout

非線形性が重みとデータとの間のドット積に適用される関数形式f（wTx + b）を有さない他のタイプのユニットが提案されている。 1つの比較的一般的な選択肢は、ReLUとそのリーキーバージョンを一般化するMaxoutニューロン（最近Goodfellowらによって導入された）である。 Maxoutニューロンは関数max（wT1x + b1、wT2x + b2）を計算する。 ReLUとLeaky ReLUの両方がこのフォームの特別なケースであることに注意してください（たとえば、ReLUにはw1、b1 = 0があります）。 したがって、Maxoutニューロンは、ReLUユニット（動作の線形制御、飽和なし）のすべての利点を享受し、その欠点（ReLUで死に至る）を持たない。 しかし、ReLUニューロンとは異なり、単一のニューロンごとにパラメータの数が倍増し、高い総数のパラメータにつながります。

これは、最も一般的なタイプのニューロンとそれらの活性化機能についての議論を終了する。 最後のコメントとして、同じネットワークにさまざまなタイプのニューロンを混在させて一致させることは、根本的な問題はないが、非常にまれである。




TLDR
「どのニューロンタイプを使用すればよいですか？」ReLU非線形性を使用し、学習率に注意し、ネットワーク内の「死んだ」ユニットの割合を監視することができます。 これが懸念される場合は、Leaky ReLUまたはMaxoutに試してみてください。 シグモイドを使用しないでください。 tanhを試してみてくださいが、ReLU / Maxoutよりも悪いと思っています。


Neural Network architectures
ニューラルネットワークアーキテクチャ

Layer-wise organization
層別組織

Neural Networks as neurons in graphs
グラフのニューロンとしてのニューラルネットワーク




