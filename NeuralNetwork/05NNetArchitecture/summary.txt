Summary

In summary,
・  我々は、生物学的ニューロンの非常に粗いモデルを導入した。

・  ReLUが最も一般的な選択である、
    実際に使用されるいくつかのタイプの活性化機能について議論した

・  ニューロンが完全接続された層と接続されたニューラルネットワークを導入した。
    ここで、隣接する層のニューロンは完全なペアワイズ接続を有するが、
    層内のニューロンは接続されていない。

・  この階層化されたアーキテクチャは、
    活性化関数の適用と織り込まれた行列乗算に基づいた
    ニューラルネットワークの非常に効率的な評価を可能にすることを見出した。

・  ニューラルネットワークはユニバーサル関数近似であることがわったが、
    このプロパティはユビキタス用途とほとんど関係がないという事実についても議論した。
    それらは実際に現れる関数型の関数型についてある程度 "正しい"仮定をするために使われる。

・  大規模なネットワークは常に小規模なネットワークよりも優れているという事実について議論したが、
    より高いモデル容量はより強力な正則化（より高い重量減など）で適切に対処しなければならないか、
    後のセクションで、より多くの形式の正規化（特にドロップアウト）を見ていく。