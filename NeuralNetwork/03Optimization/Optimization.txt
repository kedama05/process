Optimizarion

前回までで2つの重要な要素を導入した
スコア関数: 生のイメージピクセルをクラススコア（例えば線形関数）にマッピングする。
損失関数:   スコアがトレーニングデータのground truthラベルとどの程度一致しているかで、
            特定のパラメータセットの品質を測定する。
            Softmax / SVMといった多くの方法がある。

具体的には、線形関数はf（xi、W）= Wxiの形式を持ち、我々が開発したSVMは以下のように定式化された：

L　=　1/N ∑i∑j≠yi [ max( 0,f(xi;W)j − f(xi;W)yi + 1 ) ] + αR(W)

ground truthラベルyiと一致するxiの予測を生成したパラメータWの設定もまた、
非常に低い損失Lを有することを見出した。

次に、3番目および最後の重要な要素、すなわち最適化を導入する。
最適化は、損失関数を最小にするパラメータWのセットを見つけるプロセスである。

Visualizing the loss function
損失関数の可視化


このクラスで見ている損失関数は、通常、
非常に高次元の空間で定義されているため、視覚化が困難である。
しかし、光線に沿った高次元空間（1次元）または平面（2次元）に沿って
スライスすることによって、まだ1つの直観を得ることができる。
例えば、ランダム重み行列WW1L（W + aW1）aaL（W + aW1 + bW2）a、ba、bを生成できる。


式から明らかなように、各例のデータ損失は、Wの最大（0、 - ）関数による線形関数のゼロしきい値との和である。

さらに、Wの各行（wj）は、その正面に正の符号（例では間違ったクラスに対応する）を有し、
ときには負の符号を有することもある（その例では正しいクラスに対応する）。

これをより明示するには、3つの1次元点と3つのクラスを含む単純なデータセットを考える。
完全なSVM損失（正則化なし）は次のようになる。
L0 = max(0,wT1x0−wT0x0+1)+max(0,wT2x0−wT0x0+1)
L1 = max(0,wT0x1−wT1x1+1)+max(0,wT2x1−wT1x1+1)
L2 = max(0,wT0x2−wT2x2+1)+max(0,wT1x2−wT2x2+1)
L = (L0+L1+L2)/3

これらの例は1次元なので、データxiと重みwjは数値である。
たとえば、w0を見ると、上の項はw0の線形関数であり、それぞれの項はゼロでクランプされている。

スコア関数fをニューラルネットワークに拡張すると、目的関数は非凸となり、
上記の視覚化はボウルを特徴とせず、複雑で不規則な地形になります。

微分不可能な損失関数。
テクニカルノートでは、これらのねじれで勾配が定義されていないため、
損失関数のねじれ（最大操作のため）によって技術的に損失関数を微分できないことがわかる。

しかし、劣勾配(サブグラディエント)はまだ存在し、代わりに一般的に使用されている。
ここでは、劣勾配とグラデーションという用語を同じ意味で使用する。


Optimization
最適化

繰り返すが、損失関数は、重みWの任意の特定の集合の品質を定量化することを可能にする。
最適化の目的は、損失関数を最小にするWを見つけることにある。

損失機能を最適化するためのアプローチを動機付けし、ゆっくりと開発する。
以前の経験でこのクラスに来る人にとっては、使用する実際の例が凸面の問題であるため、
このセクションは奇妙に思えるかもしれないが、我々の最終的な目標は、
Convex Optimizationの文献で開発されたツールを簡単に使用できないニューラルネットワークを最適化することである。





戦略＃1：非常に悪い解決策：ランダム検索
単純に多くの異なるランダムウェイトを試し、最良のものを追跡する
    いくつかのランダムな重みベクトルWを試し、
    検索で​​見つかった最良の重みWをテストセットで試す。

与えられたパラメータWがどれほど良いかをチェックするのはとても簡単。
最高のWを使用すると、約15.5％の精度が得られる。

コアアイデア: 反復的な改良
    ランダムなWで始まり、それを反復的に改良して毎回少しずつ改善する。
        重みWの最適な集合を見つけることは、非常に困難であるが、
        重みWの特定の集合をわずかに改善するという問題は、あまり難しくない。





目の不自由なハイカーの類推
    前進するのに役立つかもしれない1つのアナロジーは、
    目の不自由な丘陵地帯でハイキングをしていると考えて、底に達することを試みること。


戦略＃2：ランダムローカル検索
    初めの戦略は、片足をランダムな方向に広げ、
    それが下り坂をリードする場合にのみステップを取ること。

    具体的には、ランダムなWで始まり、ランダムな摂動 δW を生成し、
    摂動した W + δW での損失が低い場合、更新を実行する。

    この方法は優れた精度で分類できるが、無駄で計算時間がかかる。


戦略＃3：勾配に従う
    重みベクトルを変更するための最良の方向を計算する。
    この方向は、損失関数の勾配に関連する。

    ハイキングの類推においては、
    このアプローチはおおよそ丘の斜面を私たちの足元で感じ、
    最も急な感じの方向に向かうことに対応する。

    勾配は、入力空間内の各次元の傾きのベクトル。
    df(x)/dx = limh→0 f(x+h)−f(x) / h

    関心のある関数が単一の数ではなく複数のベクトルをとるとき、
    導関数を偏導関数と呼び、勾配は単純に各次元の偏微分のベクトルです。




Computing the gradient
グラデーションの計算

勾配を計算するには2つの方法がある。

1.  数値勾配: 遅く、近似的ではあるが簡単な方法
2.  計算勾配: 正確ではあるがエラーの起こりやすい方法


1.  数値勾配
有限差分で数値的に勾配を計算する
    下記の式は、数値的に勾配を計算することを可能にする。
    df(x)/dx = limh→0 f(x+h)−f(x) / h

    数学的定式化では、hがゼロに近づくにつれて勾配が限界に定義されるが、
    非常に小さい値を使用すれば十分である。
    
    実際には、中心差分式を使用して数値勾配を計算する方が効果的。
    [f(x+h)−f(x−h)]/2h

ステップサイズ（または後に学習レートと呼ぶ）は、
慎重に調整する必要がある最も重要なハイパーパラメータの1つである。

任意の点および任意の関数の勾配を計算することができる。
しかし、数値勾配を評価することは、パラメータの数が一様に複雑である。



2.  計算勾配
微積分で解析的に勾配を計算する
    数値勾配は有限差分近似を使用して計算するのは非常に簡単であるが、欠点は近似値。
    また、計算には計算コストが非常に高い。

    微積分を分析的に使用し、計算が非常に速い勾配（近似なし）の直接式を導き出す。
    ただし、数値勾配とは異なり、実装時によりエラーが起こりやすいため、
    解析勾配を計算して数値勾配と比較して実装の正確性を確認(勾配チェック)するのが一般的。

単一のデータポイントに対してSVM損失関数の例を使用できる。
    Li=∑j≠yi[max(0,wTjxi−wTyixi+Δ)]

重みに関して関数を区別することができる。wyiに関して勾配をとると、次のようになる。
    ∇wyi Li = −( ∑j≠yi 1( wTjxi−wTyixi+Δ > 0 ) )xi

ここで、1はインジケータ関数で、内部の条件が真の場合は1、そうでない場合はゼロ。

式が書き出されるときに式が恐ろしく見えるかもしれないが、
コードでこれを実装するときは、目的のマージンを満たさない(損失関数に寄与する)クラスの数を数えてから、
データ この数だけスケーリングされたベクトルxiは勾配である。
これは、正しいクラスに対応するWの行に関してのみ勾配である。
j≠yiである他の行については、勾配は：
    ∇wjLi=1(wTjxi−wTyixi+Δ>0)xi






