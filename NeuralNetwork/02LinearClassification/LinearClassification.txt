Linear Classification

kNNにはいくつかの欠点があった。

1.  分類器は、すべてのトレーニングデータを記憶し、
    将来のテストデータとの比較のために保存する必要がある。

2.  テスト画像を分類することは、
    すべてのトレーニング画像との比較を必要とするので高価である。


Overview
ニューラルネットワークと畳み込みニューラルネットワーク全体に自然に拡張される
画像分類の強力なアプローチを開発しようとしている。

このアプローチには、スコア関数と損失関数という2つの主要な要素がある。

スコア関数: 生データをクラススコアにマッピングする
損失関数: 予測スコアとグラウンドトゥルーラベルとの間の一致を定量化する

次に、スコア関数のパラメータに関する損失関数を最小化する最適化問題としてこれをキャストする。


Parameterized mapping from images to label scores

画像のピクセル値を各クラスの信頼スコアにマッピングするスコア関数を定義する。


training dataset of images xi∈RD と過程(assume)する。
それぞれ label yi と関連付けられる。
ここで、i=1…N であり、 yi∈1…K である。
つまり、N個の例（それぞれ次元Dを持つ）とK個の異なるカテゴリがある。


CIFAR-10では、N = 50,000 imagesのトレーニングセット、それぞれD = 32 x 32 x 3 = 3072 pixelsと、 K = 10、10 distinct classes あるため。
raw image pixels をクラススコアにマッピングするスコア関数 f:RD↦RK を定義する。

Linear classifier.
このモジュールでは、間違いなく可能な最も単純な関数、線形マッピングから始める。
f(xi,W,b)=Wxi+b

上式では、image xi はすべてのピクセルをshape [D x 1]のシングルカラムベクトルにフラット化したものと仮定する。
マトリックス W (of size [K x D]) とベクトル b (of size [K x 1])は、ファンクションのparametersである。
CIFAR-10では、 xi はi-th image を[3072 x 1]にフラット化したすべてのピクセルを含み、
Wは[10 x 3072]、bは[10 x 1]なので、3072の数値(the raw pixel values)が関数に入り、10の数値 (the class scores).が出力されます。。

パラメータの Wは重み、bはバイアスベクトルと呼ばれる。
これは出力スコアに影響するが、実際のデータxiとは相互作用しないためである。
しかし、しばしば重みとパラメータの用語を同じ意味で使用する。


注意すべき点:
まず、1つの行列乗算 Wxi が、各分類器がWの行である10個の別個の分類器を並列に（各クラスに1つ）効果的に評価することに留意されたい。
（x私、y私）
このアプローチの利点は、学習データがパラメータW、bを学習するために使用されるが、学習が完了したら学習セット全体を破棄し、学習されたパラメータのみを保持できることである。これは、新しいテスト画像を単純に機能を通して転送し、計算されたスコアに基づいて分類することができるからである。

最後に、テスト画像を分類するには、行列の乗算と加算を1回行う必要がある。
これは、テスト画像とすべてのトレーニング画像を比較するよりもはるかに高速である。


Interpreting a linear classifier

線形分類器は、クラスのスコアを、すべてのピクセル値において、その3つすべてのカラーチャネルの加重和として計算する。

Wの行の1つを変更すると、ピクセル空間内の対応する線が異なる方向に回転すること。
一方、バイアスbは、分類器がラインを移動することを可能にする。
バイアス項がないと、x i = 0を差し込むと重みにかかわらず常にスコアがゼロになる。

重みWの別の解釈は、Wの各行がクラスのうちの1つのテンプレート（時にプロトタイプと呼ばれる）に対応することである。
次に、各テンプレートを画像と比較して、内部プロダクト（またはドットプロダクト）を1つずつ使用して、
「適合する」ものを見つけることによって、画像の各クラスのスコアが得られる。

この方法では、線形分類器はテンプレートマッチングを行っており、そこでテンプレートが学習される。


もう一つの方法: Nearest Neighborを効果的にやる。

たくさんのトレーニング画像を使用する代わりに、クラスごとに1つの画像のみを使用する。
(学習は行うが、必ずしもトレーニングセットの画像の1つである必要はない)
L1またはL2距離の代わりに（負の）内積を距離として使用する。


ベクトルxiに定数1を固定で追加し、Wとbをセットにして一つのパラメータとして置き換えることで、
スコア関数は以下のように書き直せる。
f(xi,W,b)=Wxi+b → f(xi,W)=Wxi

つまり、すべての入力ベクトルに1を追加してデータを前処理すると、1つの重み行列を学習するだけで済む。

Image data preprocessing.
機械学習では、入力フィーチャを正規化を常に実行することは非常に一般的な方法である。
特に、すべてのフィーチャから平均を差し引いてデータをセンタリングすることは重要である。

さらに一般的な前処理では、各入力フィーチャの値が[-1、1]の範囲になるように各入力フィーチャをスケールする。
これらのうち、ゼロ平均センタリングはおそらくより重要ですが、勾配降下のダイナミクスを理解するまで、その正当化を待たなければならない。