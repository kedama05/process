Summary

・  グラディエントの意味、回路内でどのように後方に流れるか、
    回路のどの部分が増減すべきか、
    
・  バックプロパゲーションの実際的な実装のための段階的計算の重要性について議論した。
    関数をモジュールに分解して、局所的な勾配を簡単に導出し、
    連鎖ルールで連鎖させることができる。
    重要なのは、入力変数の勾配に明示的な数学的方程式を使う必要がないため、
    これらの式を紙に書き、それらを完全に象徴的に区別することはほとんどない。
    したがって、式を段階的に分解して、各段階を個別に区別することができる。
    （段階は行列ベクトルの乗算、最大演算、合計演算など）


次のセクションでは、ニューラルネットワークの定義を開始し、バックプロパゲーションにより、
損失関数に関してニューラルネットワークの接続におけるグラディエントを効率的に計算できる。
言い換えれば、ニューラルネットを訓練する準備が整った。
このクラスの最も概念的に難しい部分がもうすぐ来る。ConvNetsまであと少し。