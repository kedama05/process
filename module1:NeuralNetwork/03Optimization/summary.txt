
Summary

情報の流れの概要。
（x、y）の対のデータセットが与えられ、固定される。
重みは乱数として始まり、変更することができる。
フォワードパスの間、スコア関数は、ベクトルfに格納されたクラススコアを計算する。
損失関数には2つの要素が含まれる。
データ損失はスコアfとラベルyの間の互換性を計算する。
正則化損失は、重みの関数にすぎない。
Gradient Descentでは、ウェイトのグラディエントを（必要に応じてデータ上にオプションで）計算し、
Gradient Descentの間にパラメータを更新するために使用する。

In this section,
損失関数を、高次元の最適化環境として開発した。
そこでは、最下位に到達しようとしる。

・  私たちが開発した類推作業は、目が見えないハイカーがふもとにたどり着こうとするようなものだった。
    特に、SVMコスト関数は、区分的線形でボウル形状であることがわかった。

・  我々は、反復微調整を用いて損失関数を最適化するという考え方に動機づけた 。
    ここでは、無作為な一組の重みから始めて、損失が最小になるまで段階的に微調整する。

・  関数の勾配が最も急な上昇方向を示すことを見出し、
    有限差分近似（有限差分は数値勾配の計算に使用されるhの値）を使用して
    数値的に計算する単純ではあるが非効率的な方法を検討した。

・  パラメータの更新には、ステップサイズ（または学習率）を慎重に設定する必要があることがわかった。
    これは、正しく設定する必要がある。
    低すぎると、進行は安定していますが遅くなる。
    高すぎると、進歩はより速くなるが、より危険。
    このトレードオフについては、今後のセクションで詳しく説明する。

・  数値勾配と解析勾配の計算との間のトレードオフについて議論した。
    数値的な勾配は単純だが、近似して計算するのに費用がかかる。
    解析勾配は正確で計算が速いが、誤差の傾向があるのは、勾配を数学で導出する必要があるため。
    したがって、実際には、常に分析グラディエントを使用して、グラジエントチェックを実行する。
    グラジエントチェックでは、その実装を数値グラジェントと比較する。

・  勾配を繰り返し計算し、ループでパラメータ更新を実行するGradient Descentアルゴリズムを導入した。



