Loss function
http://cs231n.github.io/linear-classify/#loss

前節では、ピクセル値からクラススコアへの関数を定義した。


損失関数を用いて、結果の良し悪しを推定する。
訓練データの分類が貧弱であれば損失は高くなり、うまくいけば低くなる。



損失機能の詳細を定義する方法はいくつかある。

Multiclass Support Vector Machine (SVM)損失
SVM損失は、SVMが、各画像の正しいクラスが、
不正確なクラスよりも高いスコアをいくらかの固定マージンΔだけ有するように設定される。

SVMは、結果がより低い損失をもたらすという意味で一定の結果を望んでいる（これは良好である）。

j番目のクラスのスコアはj番目の要素は sj=f(xi,W)j. となる。
また、i番目の例のMulticlass SVM損失を次のように定式化する。
Li=∑_j≠yi max(0,sj−syi+Δ)

要約すると、SVM損失関数は、
正しいクラスyiのスコアが誤ったクラススコアより少なくともΔだけ大きいことを望み、
そうでない場合は、損失を累積する。
(差がΔ以上の場合にはゼロにクランプする。)

この特定のモジュールでは、線形スコア関数s( f(xi;W) = Wxi )を使用しているので、
この等価な形式で損失関数を書き直すこともできる。
Li=∑_j≠yi max(0, wTj xi−wTyi xi + Δ)
ここでwjは列として再構成された Wのj番目の行


zero max(0、 - )関数でのスレッショルドがしばしばヒンジ損失と呼ばれる。
ヒンジ損失SVMや、L2-SVMを用いる代わりに、
違反したマージンをより強く罰する形式である、max（0、 - ）^2を用いることがある。

平方されていないバージョンがより標準だが、
一部のデータセットではヒンジの平方和のようがより良いことがある。

Multiclass Support Vector Machineは、
正しいクラスのスコアが他のすべてのスコアよりも少なくともデルタのマージン分だけ高くなることを望んでいます。
いずれかのクラスに赤い領域（またはそれ以上）のスコアがある場合、累積損失があります。
それ以外の場合、損失はゼロになります。
私たちの目的は、トレーニングデータのすべての例についてこの制約を同時に満たし、
可能な限り低い総損失を与える重みを見つけることです。




Regularization: 正規化

上記の損失関数には1つのバグがある。
正しい分類を行えるWのセットは、必ずしも一意であるとは限らず、類似のWが多数存在する可能性がある。

すべての例で損失関数がゼロとなるWを定数倍したものもまた、損失関数はゼロとなる。
(この変換はすべてのスコアの大きさを一様に伸ばし、その絶対差も均一に伸ばすため)

損失関数を正規化ペナルティR（W）で拡張することで、
重みWのあるセットを他のものよりも優先させ、あいまいさを除去することができる。

最も一般的な正則化のペナルティはL2ノルムであり、
すべてのパラメータよりも要素的な2次ペナルティによって大きな重みが失われる。
R(W)=∑_k∑_l W2k,l


上記の式では、Wのすべての二乗要素を集計している。
正則化関数はデータの関数ではなく、重みにのみ基づいている。
正則化ペナルティを含めると、データ損失（すべての例よりも平均損失Li）と
正則化損失の2つの要素で構成される完全なMulticlass Support Vector Machineの損失が完成する。
つまり、完全なMulticlass SVM損失は次のようになる。
L= 1/N ∑_i Li + λR(W)

これを展開すると以下の形になる。
L= 1/N ∑_i ∑_j≠yi [ max(0, f(xi;W)_j − f(xi;W)_y_i + Δ) ] + λ ∑_k ∑_l W2k,l

ここでNはトレーニング回数を表す。
ハイパーパラメータλで重み付けされた損失目的に正則化ペナルティを追加する。
このハイパーパラメータを設定する簡単な方法はなく、
通常はクロスバリデーションによって決定される。

上記の動機づけに加えて、正則化のペナルティを含めることが望ましい特性が多くある。
たとえば、L2ペナルティを含めると、SVMの魅力的な最大マージンプロパティが得られることが分かる。

 L2ペナルティは、より小さいかつより拡散した重みベクトルを好むので、
 最終的な分類器は、少数の入力次元ではなく、
 非常に強く、すべての入力次元を少量に考慮することが推奨される。


ラスの後半で説明するように、この効果は、
テスト画像上の分類子の一般化性能を向上させ、
オーバーフィットを少なくすることができる。


バイアスは、重みとは異なり、入力ディメンションの影響力を制御しないため、バイアスは正規化しないことが一般的。
正規化ペナルティのために、すべての例で正確に0.0の損失を達成することはできないことに留意されたい。
(W = 0という異常な設定でのみ可能であるため)


私たちが今やっておかなければならないことは、損失を最小限に抑えるウェイトを見つける方法を考え出すことだけです。

ハイパーパラメータΔとλは2つの異なるハイパーパラメータのように見えるが、
実際には両方とも同じデータ損失と目的の正規化損失のトレードオフを制御する。


理解のための鍵: 重みWの大きさがスコア（およびその差）に直接影響を及ぼすということ。
W内のすべての値を小さくすると、スコアの差が小さくなり、
重みを大きくするとスコア相違点はすべて高くなる。
    ↓
スコア間のマージンΔの正確な値（例えば、Δ= 1またはΔ= 100）は、
重みが任意に差を縮小または伸長することができるため、無意味。
    ⇓
唯一の実際のトレードオフは、重みが（正則化強度λを介して）どのくらい大きく成長するかである。


Multiclass SVM: 複数のクラスにわたってSVMを定式化するいくつかの方法の1つ
One-Vs-All（OVA）SVM: 各クラスと他のすべてのクラスに対して独立したバイナリSVMを訓練する
Structured SVM: 正しいクラスのスコアと最高得点の間違ったランナークラスの得点との間のマージンを最大化する





Softmax classifier

Softmax classifier:
    SVMと異なる損失関数を有する分類器
    バイナリロジスティック回帰分類を複数クラスに対応するように一般化したもの。
    出力f（xi、W）を各クラスの得点として扱うSVMとは異なり、
    Softmax分類器はやや直感的な出力を与え、確率論的解釈も短時間で行う。

    Softmax分類器では、関数写像f（xi; W）= Wxiは変化しないままであるが、
    これらのスコアを各クラスの非正規化対数確率として解釈し、
    ヒンジ損失を以下の形式のクロスエントロピー損失で置き換える。

    Li = −log( e^f_yi / ∑_j e^f_j )  or equivalently  Li = −f_yi + log ∑_j e^fj


fjはクラススコアのベクトルfのj番目の要素を意味している。
前述のように、データセットの全損失は、すべてのトレーニング例のLiの平均と正規化項R（W）である。
関数fj（z）=e^z_j / Σ_k e^z_k はsoftmax関数と呼ばれ、
任意の実数値スコア（z）のベクトルをとり、ゼロと1の和のベクトルの値にスカッシュする。
ソフトマックス機能を含む完全なクロスエントロピー損失は、動機づけは比較的簡単。


「真の」分布pと推定分布qとの間のクロスエントロピーは、以下のように定義される。
H（p、q）= - Σxp（x）logq（x）

したがって、Softmax分類器は、推定されたクラス確率（上記のようにq = e^f_y_i / Σ_j e^f_j）と
「真の」分布との間のクロスエントロピーを最小化する。

この解釈では、すべての確率集合が正しいクラス(すなわち、p = [0、... 1、...、0]は、yi番目の位置に単一の1を含む。)
さらに、クロスエントロピーはエントロピーとKullback-Leiblerの発散で
H（p、q）= H（p）+ D_KL（p || q）と書くことができ、
デルタ関数pのエントロピーがゼロである場合、
これは2つの分布（距離の尺度）間のKL発散を最小限にすることと同等である。

言い換えれば、クロスエントロピーの目的は、
予測された分布が正解のすべての集合を有することである。



Probabilistic interpretation
確率論的解釈

「真の」分布と推定分布との間のクロスエントロピーは、以下のように定義される。
P(yi∣xi;W) = e^fyi ∑j e^fj

この式は、画像xiが与えられ、
Wによってパラメータ化された正しいラベルyiに割り当てられた（正規化された）確率として解釈できる。
Softmax分類器が出力ベクトルf内のスコアを非正規化対数確率として解釈する。
    →これらの量を累乗すると（正規化されていない）確率が得られ、除算は確率が1になるように正規化を実行する。

    →確率論的解釈では、正しいクラスの負の対数尤度を最小化している。
    これは、最尤推定（MLE）を実行すると解釈することができる。

この見解の優れた点は、
全損失関数の正規化項R(W)を重み行列Wよりも先にガウス分布から来るものとして解釈できることであり、
ここで、MLEの代わりに最大事後（MAP）推定を実行している。


Practical issues: Numeric stability
実用上の問題：数値安定性

Softmax関数を実際に計算するためのコードを書くとき、
中間項efyiとΣjefjは指数関数のために非常に大きくなることがある。
大きな数値を分割すると数値的に不安定になる可能性があるので、正規化のトリックを使用することが重要。

e^fyi / ∑_j e^fj = C e^fyi / C ∑_j e^fj = e^(fyi + log C) / ∑_j e^(fj + logC)

分母分子を定数倍することで式を変形し、計算の数値的安定性を向上させることができる。

Cは、 logC = -max_j fj と設定するのが一般的。
これは、最高値がゼロになるようにする必要があるという意味。



Possibly confusing naming conventions おそらく混乱を招く命名規則

SVM classifier: ヒンジ損失(最大マージン損失とも呼ぶ)を用いる
Softmax classifier: クロスエントロピー損失を用いる。
        Softmax classifierでは、未処理のクラススコアを合計して正規化された正の値に縮めて、
        クロスエントロピー損失を適用できるようにするためにSoftmax関数を使用する。



SVM vs. Softmax

1つのデータポイントのSVMとSoftmaxの違いの例
どちらの場合も、同じスコアベクトルfを計算する。
違いは、fのスコアの解釈にある。
    SVM
        f: クラススコアとして解釈
        損失関数: 正しいクラスが他のクラススコアよりもマージン分だけ高いスコアを持つことを奨励する。
    Softmax分類器:
        f: 各クラスの対数確率として解釈
        損失関数: 正しいクラスの（正規化された）対数確率を高くすることを奨励する。
    
各損失関数により算出された損失の値は比較できず、
同じ分類子内かつ同じデータで計算された損失との関連でのみ意味がある。

Softmax classifier provides “probabilities” for each class. Softmax分類器は、各クラスに対して「確率」を提供する

Softmax分類器では、すべてのラベルに対して「確率」を計算することができ、各クラスの信頼度を解釈できる。
しかし、Softmax分類器によって計算される確率は、スコアの順序付けは解釈可能であるが、
絶対的な数は技術的ではない信頼度としてよりよく考えられる。(SVMと同様に)



In practice, SVM and Softmax are usually comparable. 
実際には、SVMとSoftmaxは通常同等

SVMとSoftmaxの性能差は通常非常に小さい。
Softmax分類器と比較して、SVMはより局所的な目的であり、バグや機能のいずれかと考えることができる。

SVMは、正しいクラスが既に他のクラスと比較してマージンよりも高いスコアを有し、
ゼロの損失を計算することを見るため、個々の得点の詳細を気にしない。

SVMではマージンが満たされれば満足し、この制約を超えて正確なスコアを追求しないが、
Softmax分類器は、決して得点に満足することがない。
正しいクラスは常に高い確率を持ち、不正クラスは常に低い確率を持ち、損失は常に良くなる。


これは、直感的に特徴として考えることができます。
例えば、車からトラックを分離するという困難な問題に対してほとんどの "努力"を費やしそうな車の分類子は、カエルの例の影響を受けてはいけません。
データクラウドとはまったく異なる側面に集中する可能性があります。