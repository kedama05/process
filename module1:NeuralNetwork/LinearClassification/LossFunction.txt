Loss function
http://cs231n.github.io/linear-classify/#loss

前節では、ピクセル値からクラススコアへの関数を定義した。


損失関数を用いて、結果の良し悪しを推定する。
訓練データの分類が貧弱であれば損失は高くなり、うまくいけば低くなる。



損失機能の詳細を定義する方法はいくつかある。

Multiclass Support Vector Machine (SVM)損失
SVM損失は、SVMが、各画像の正しいクラスが、
不正確なクラスよりも高いスコアをいくらかの固定マージンΔだけ有するように設定される。

SVMは、結果がより低い損失をもたらすという意味で一定の結果を望んでいる（これは良好である）。

j番目のクラスのスコアはj番目の要素は sj=f(xi,W)j. となる。
また、i番目の例のMulticlass SVM損失を次のように定式化する。
Li=∑_j≠yi max(0,sj−syi+Δ)

要約すると、SVM損失関数は、
正しいクラスyiのスコアが誤ったクラススコアより少なくともΔだけ大きいことを望み、
そうでない場合は、損失を累積する。
(差がΔ以上の場合にはゼロにクランプする。)

この特定のモジュールでは、線形スコア関数s( f(xi;W) = Wxi )を使用しているので、
この等価な形式で損失関数を書き直すこともできる。
Li=∑_j≠yi max(0, wTj xi−wTyi xi + Δ)
ここでwjは列として再構成された Wのj番目の行


zero max(0、 - )関数でのスレッショルドがしばしばヒンジ損失と呼ばれる。
ヒンジ損失SVMや、L2-SVMを用いる代わりに、
違反したマージンをより強く罰する形式である、max（0、 - ）^2を用いることがある。

平方されていないバージョンがより標準だが、
一部のデータセットではヒンジの平方和のようがより良いことがある。

Multiclass Support Vector Machineは、
正しいクラスのスコアが他のすべてのスコアよりも少なくともデルタのマージン分だけ高くなることを望んでいます。
いずれかのクラスに赤い領域（またはそれ以上）のスコアがある場合、累積損失があります。
それ以外の場合、損失はゼロになります。
私たちの目的は、トレーニングデータのすべての例についてこの制約を同時に満たし、
可能な限り低い総損失を与える重みを見つけることです。




Regularization: 正規化

上記の損失関数には1つのバグがある。
正しい分類を行えるWのセットは、必ずしも一意であるとは限らず、類似のWが多数存在する可能性がある。

すべての例で損失関数がゼロとなるWを定数倍したものもまた、損失関数はゼロとなる。
(この変換はすべてのスコアの大きさを一様に伸ばし、その絶対差も均一に伸ばすため)

損失関数を正規化ペナルティR（W）で拡張することで、
重みWのあるセットを他のものよりも優先させ、あいまいさを除去することができる。

最も一般的な正則化のペナルティはL2ノルムであり、
すべてのパラメータよりも要素的な2次ペナルティによって大きな重みが失われる。
R(W)=∑_k∑_l W2k,l


上記の式では、Wのすべての二乗要素を集計している。
正則化関数はデータの関数ではなく、重みにのみ基づいている。
正則化ペナルティを含めると、データ損失（すべての例よりも平均損失Li）と
正則化損失の2つの要素で構成される完全なMulticlass Support Vector Machineの損失が完成する。
つまり、完全なMulticlass SVM損失は次のようになる。
L= 1/N ∑_i Li + λR(W)

これを展開すると以下の形になる。
L= 1/N ∑_i ∑_j≠yi [ max(0, f(xi;W)_j − f(xi;W)_y_i + Δ) ] + λ ∑_k ∑_l W2k,l

ここでNはトレーニング回数を表す。
ハイパーパラメータλで重み付けされた損失目的に正則化ペナルティを追加する。
このハイパーパラメータを設定する簡単な方法はなく、
通常はクロスバリデーションによって決定される。

上記の動機づけに加えて、正則化のペナルティを含めることが望ましい特性が多くある。
たとえば、L2ペナルティを含めると、SVMの魅力的な最大マージンプロパティが得られることが分かる。

 L2ペナルティは、より小さいかつより拡散した重みベクトルを好むので、
 最終的な分類器は、少数の入力次元ではなく、
 非常に強く、すべての入力次元を少量に考慮することが推奨される。


ラスの後半で説明するように、この効果は、
テスト画像上の分類子の一般化性能を向上させ、
オーバーフィットを少なくすることができる。


バイアスは、重みとは異なり、入力ディメンションの影響力を制御しないため、バイアスは正規化しないことが一般的。
正規化ペナルティのために、すべての例で正確に0.0の損失を達成することはできないことに留意されたい。
(W = 0という異常な設定でのみ可能であるため)


私たちが今やっておかなければならないことは、損失を最小限に抑えるウェイトを見つける方法を考え出すことだけです。

ハイパーパラメータΔとλは2つの異なるハイパーパラメータのように見えるが、
実際には両方とも同じデータ損失と目的の正規化損失のトレードオフを制御する。


理解のための鍵: 重みWの大きさがスコア（およびその差）に直接影響を及ぼすということ。
W内のすべての値を小さくすると、スコアの差が小さくなり、
重みを大きくするとスコア相違点はすべて高くなる。
    ↓
スコア間のマージンΔの正確な値（例えば、Δ= 1またはΔ= 100）は、
重みが任意に差を縮小または伸長することができるため、無意味。
    ⇓
唯一の実際のトレードオフは、重みが（正則化強度λを介して）どのくらい大きく成長するかである。


Multiclass SVM: 複数のクラスにわたってSVMを定式化するいくつかの方法の1つ
One-Vs-All（OVA）SVM: 各クラスと他のすべてのクラスに対して独立したバイナリSVMを訓練する
Structured SVM: 正しいクラスのスコアと最高得点の間違ったランナークラスの得点との間のマージンを最大化する





Softmax classifier



