Loss function
http://cs231n.github.io/linear-classify/#loss

前節では、ピクセル値からクラススコアへの関数を定義した。


損失関数を用いて、結果の良し悪しを推定する。
訓練データの分類が貧弱であれば損失は高くなり、うまくいけば低くなる。



損失機能の詳細を定義する方法はいくつかある。

Multiclass Support Vector Machine (SVM)損失
SVM損失は、SVMが、各画像の正しいクラスが、
不正確なクラスよりも高いスコアをいくらかの固定マージンΔだけ有するように設定される。

SVMは、結果がより低い損失をもたらすという意味で一定の結果を望んでいる（これは良好である）。

j番目のクラスのスコアはj番目の要素は sj=f(xi,W)j. となる。
また、i番目の例のMulticlass SVM損失を次のように定式化する。
Li=∑_j≠yi max(0,sj−syi+Δ)

要約すると、SVM損失関数は、
正しいクラスyiのスコアが誤ったクラススコアより少なくともΔだけ大きいことを望み、
そうでない場合は、損失を累積する。
(差がΔ以上の場合にはゼロにクランプする。)

この特定のモジュールでは、線形スコア関数s( f(xi;W) = Wxi )を使用しているので、
この等価な形式で損失関数を書き直すこともできる。
Li=∑_j≠yi max(0, wTj xi−wTyi xi + Δ)
ここでwjは列として再構成された Wのj番目の行


zero max(0、 - )関数でのスレッショルドがしばしばヒンジ損失と呼ばれる。
ヒンジ損失SVMや、L2-SVMを用いる代わりに、
違反したマージンをより強く罰する形式である、max（0、 - ）^2を用いることがある。

平方されていないバージョンがより標準だが、
一部のデータセットではヒンジの平方和のようがより良いことがある。

Multiclass Support Vector Machineは、
正しいクラスのスコアが他のすべてのスコアよりも少なくともデルタのマージン分だけ高くなることを望んでいます。
いずれかのクラスに赤い領域（またはそれ以上）のスコアがある場合、累積損失があります。
それ以外の場合、損失はゼロになります。
私たちの目的は、トレーニングデータのすべての例についてこの制約を同時に満たし、
可能な限り低い総損失を与える重みを見つけることです。




Regularization: 正規化

上記の損失関数には1つのバグがある。
正しい分類を行えるWのセットは、必ずしも一意であるとは限らず、類似のWが多数存在する可能性がある。

すべての例で損失関数がゼロとなるWを定数倍したものもまた、損失関数はゼロとなる。
(この変換はすべてのスコアの大きさを一様に伸ばし、その絶対差も均一に伸ばすため)

損失関数を正規化ペナルティR（W）で拡張することで、
重みWのあるセットを他のものよりも優先させ、あいまいさを除去することができる。

最も一般的な正則化のペナルティはL2ノルムであり、
すべてのパラメータよりも要素的な2次ペナルティによって大きな重みが失われる。
R(W)=∑_k∑_l W2k,l


上記の式では、Wのすべての二乗要素を集計している。
正則化関数はデータの関数ではなく、重みにのみ基づいている。
正則化ペナルティを含めると、データ損失（すべての例よりも平均損失Li）と
正則化損失の2つの要素で構成される完全なMulticlass Support Vector Machineの損失が完成する。
つまり、完全なMulticlass SVM損失は次のようになる。
L= 1/N ∑_i Li + λR(W)

これを展開すると以下の形になる。
L= 1/N ∑_i ∑_j≠yi [ max(0, f(xi;W)_j − f(xi;W)_y_i + Δ) ] + λ ∑_k ∑_l W2k,l

ここでNはトレーニング回数を表す。
ハイパーパラメータλで重み付けされた損失目的に正則化ペナルティを追加する。
このハイパーパラメータを設定する簡単な方法はなく、
通常はクロスバリデーションによって決定される。




