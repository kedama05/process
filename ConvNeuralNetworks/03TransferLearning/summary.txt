転送学習

十分なサイズのデータ​​セットを持つことは比較的まれ。
    →畳み込みネットワーク全体をゼロからランダムに初期化することはほとんどない。


代わりに、
大規模なデータセットでConvNetをプリトレインし、ConvNetを初期タスクとして使用するか、
目的のタスクの固定フィーチャ抽出プログラムとして使用する。

3つの主要な転送学習のシナリオは次のとおり

1.  固定された特徴抽出器としてのConvNet

    ImageNetで事前に準備されたConvNetを使用し、
    最後に全結合層（レイヤーの出力は別のタスクの1000クラスのスコア）を削除し、
    残りのConvNetを新しいデータセットの固定フィーチャ抽出ツールとして扱う。
    AlexNetでは、これは、分類子の直前の隠れ層の活性化を含むすべての画像の4096-Dベクトルを計算する。
    これらの機能をCNNコードと呼ぶ。
    これらのコードが（通常はそうであるように）ImageNet上のConvNetのトレーニング中に閾値設定されている場合、
    これらのコードがReLUd（つまりゼロで閾値設定されている）であることがパフォーマンスにとって重要である。
    すべての画像に対して4096-Dコードを抽出したら、新しいデータセットの線形分類器（たとえば線形SVMまたはSoftmax分類器）を訓練する。

2.  ConvNetの微調整

    第2の戦略は、新しいデータセット上のConvNetの上でクラシファイアを置き換えて再トレーニングするだけでなく、
    バ​​ックプロパゲーションを継続することによって事前トレーニングされたネットワークの重みを微調整することである。
    ConvNetのすべてのレイヤーを細かく調整することも可能。
    また、以前のレイヤーのいくつかを固定しておくこともできる。（オーバーフィッティングのため）
    ネットワークの一部の高レベル部分のみを微調整する。
    これは、ConvNetの初期の機能には、
    多くのタスクに役立つはずのより一般的な機能（エッジ検出器やカラーブロブ検出器など）が含まれているという観測によって動機づけられるが、
    ConvNetの後の層は、元のデータセットに含まれている。

    例えば、多くの犬種を含むImageNetの場合、
    ConvNetの表現力のかなりの部分は、
    犬種間の差別化に特有の特徴に専念することができる。

3.  事前訓練されたモデル

    現代のConvNetsはImageNetの複数のGPUを訓練するのに2〜3週間かかるので、
    微調整のためにネットワークを使用できる他の人の利益のために最終的なConvNetチェックポイントを公開するのが一般的である。
    たとえば、Caffeライブラリには、ネットワーク重みを共有するモデル動物園がある。


どのように微調整するのか
    ConvNetの機能は、初期のレイヤではより一般的で、後のレイヤでは固有のデータセット固有のものである。

    1.  新しいデータセットは小さく、元のデータセットに似ている。
        データが小さいので、ConvNetを過度の懸念のために微調整することは良い考えではない。
        データは元のデータと似ているため、ConvNetの上位レベルの機能もこのデータセットに関連すると考えられる。
        したがって、CNNコード上で線形分類器を訓練することが最善の考えであるかもしれない。

    2.  新しいデータセットは大きく、元のデータセットに似ている。
        より多くのデータがあるので、完全なネットワークを微調整しようとすれば、
        過度に過ぎないと確信できる。

    3.  新しいデータセットは小さく、元のデータセットとは大きく異なる。
        データは小さいので、線形分類器を訓練することが最もよいと思われる。
        データセットは非常に異なるため、
        データセット固有の機能が多く含まれているネットワークの上部にある分類子を訓練するのが最良ではない場合がある。
        代わりに、SVMクラシファイアをネットワークの以前のどこかのアクティベーションから訓練するほうが効果的かもしれない。

    4.  新しいデータセットは大きく、元のデータセットとは大きく異なる。
        データセットが非常に大きいので、私たちはConvNetを最初から訓練する余裕があると期待するかもしれない。
        しかし、実際には、あらかじめトレーニングされたモデルからウェイトを初期化することは非常にしばしば有益である。
        この場合、ネットワーク全体を微調整するのに十分なデータと信頼性がある。


実用的なアドバイス

事前訓練されたモデルからの制約
    事前に訓練されたネットワークを使用する場合は、
    新しいデータセットに使用できるアーキテクチャに関してわずかに制約されることがある。
    たとえば、事前に作成されたネットワークからConvレイヤを任意に取り出すことはできない。
    しかし、いくつかの変更は簡単：
        パラメータ共有のために、異なる空間サイズの画像で事前にネットワークを簡単に実行できる。
        これは、Conv / Poolレイヤの場合、フォワード関数が入力ボリュームの空間サイズとは独立しているため、
        （ストライドが「フィット」している限り）明らかに明らかである。
        FCレイヤーの場合、FCレイヤーを畳み込みレイヤーに変換できるため、これは当てはまる。
        たとえば、AlexNetでは、最初のFCレイヤーより前の最後のプールボリュームは[6x6x512]である。
        従って、このボリュームを見るFC層は、
        受信フィールドサイズが6×6である畳み込みレイヤを有することと等価であり、
        0のパディングで適用される。



学習率
    新しいデータセットのクラススコアを計算する新しい線形分類器の（ランダムに初期化された）重みと比較して、
    微調整されているConvNet重みの学習率を小さくするのが一般的。
    これは、ConvNetの重みが比較的良好であると予想しているため、
    あまりにも速く、あまりにも歪ませたくない。
    （特に、それらの上の新しい線形クラシファイアがランダム初期化から訓練されている間）