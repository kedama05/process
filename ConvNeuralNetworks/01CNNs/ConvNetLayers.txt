Convolutional Neural Networks
畳み込みニューラルネットワーク

畳み込みニューラルネットワークは、前の章の通常のニューラルネットワークと非常によく似ている。
学習可能な重みと偏りを持つニューロンで構成されている。

各ニューロンは、
いくつかの入力を受け取り、ドット積を実行し、任意に、それを非線形性で追跡する。

ネットワーク全体は、
一方の生の画像ピクセルから他方のクラススコアまで、
単一の微分可能なスコア関数を依然として表現している。

最後の（完全に接続された）層にはまだ損失関数（例えばSVM / Softmax）があり、
通常のニューラルネットワークを学習するために開発したすべてのヒント/トリックは依然として適用される。

ConvNetのアーキテクチャは、入力が画像であるという明示的な前提を定めているため、
アーキテクチャに特定のプロパティをエンコードすることができる。
→   フォワード機能の実装効率が向上し、ネットワーク内のパラメータ量が大幅に削減される。

各隠れ層は、
各ニューロンが前の層のすべてのニューロンに完全に接続されており、
単一の層のニューロンが完全に独立して機能し、
接続を共有していないニューロンのセットで構成されています。

最後に完全に接続されたレイヤは「出力レイヤ」と呼ばれ、分類設定ではクラススコアを表す。

レギュラーニューラルネットは、フルイメージには適していない。



Layers used to build ConvNets
ConvNetsの構築に使用されるレイヤー

単純なConvNetは一連のレイヤーであり、
ConvNetの各レイヤーは1つのボリュームのアクティベーションを異なるものに変換する。

畳み込みレイヤー、プールレイヤー、完全連結レイヤー（通常のニューラルネットワークとまったく同じ）、
3種類のレイヤーを使用してConvNetアーキテクチャーを構築する。

これらのレイヤーをスタックして、完全なConvNet アーキテクチャを形成する。



ConvNetアーキテクチャは、最も単純なケースでは、画像ボリュームを出力ボリュームに変換するレイヤーのリスト（例えば、クラススコアを保持する）
レイヤーにはいくつかの異なるタイプがある（たとえば、CONV / FC / RELU / POOLが最も人気がある）
各レイヤーは入力された3Dボリュームを受け取り、それを出力可能な3Dボリュームに変換することができる
各レイヤーにはパラメータがある場合とない場合がある（例えば、CONV / FC do、RELU / POOLはない）
各レイヤーには追加のハイパーパラメータがある場合とない場合がある（たとえば、CONV / FC / POOL do、RELUはない）


Convolutional Layer
畳み込みレイヤー

CONVレイヤのパラメータは、学習可能なフィルタのセットで構成されている。





プール層
ConvNetアーキテクチャでは、連続したConvレイヤの間に定期的にPoolingレイヤーを挿入するのが一般的。
    表現の空間サイズを漸進的に縮小して、
    ネットワークにおけるパラメータおよび計算の量を減らし、ひいては誇張を制御する。

プーリングレイヤーは、
    入力の各深度スライス上で独立して動作し、
    MAX操作を使用して空間的にサイズを変更する。

最も一般的な形式は、サイズが2x2のフィルタが適用されたプーリング層。
Poolingレイヤーにゼロパディングを使用するのは一般的ではない。


プールを取り除く
CONVレイヤーでより大きなストライドを使用することを推奨している。

プール・レイヤーを破棄することは、
バラエティオート・エンコーダー（VAE）または生成的敵対ネットワーク（GAN）などの
良い生成モデルを訓練する上で重要である。



ReLU
マイナスの数字が生じた時に、それを0に変換する。
学習値が0付近で立ち往生したり無限に向かったりすることがなくなり、CNNの健全性が保たれる。


正規化レイヤー



全結合層
    全結合層も、出力（投票のリスト）と入力（値のリスト）は全体的に類似しているため、
    その他の層と同じように積み重ねることができる。
    実際に複数の全結合層が一緒に積み重ねられることは少なくない。




ConvNetアーキテクチャ

    畳み込みネットワークは、
    CONV、POOL（別途明記されていない限りMaxプールとみなされます）と
    FC（完全接続の場合）との3種類のレイヤーで構成されている。

    RELU起動関数を要素的な非線形性を適用するレイヤーとして明示的に記述する。

レイヤパターン
    ConvNetアーキテクチャの最も一般的な形式
        いくつかのCONV-RELUレイヤーを積み重ね、POOLレイヤーに従い、
        画像が空間的に小さなサイズにマージされるまでこのパターンを繰り返す。

    ある時点では、完全に接続されたレイヤーに移行するのが一般的。
    最後に完全に接続されたレイヤーは、クラススコアなどの出力を保持する。

    つまり、最も一般的なConvNetアーキテクチャは、次のパターンに従う。
        INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC

        *: 反復, POOL?: オプションのプール層, 
        N >= 0 (and usually N <= 3), M >= 0, K >= 0

    ConvNetの一般的なアーキテクチャー
    ・  INPUT -> FC線形分類器を実装しています。ここでN = M = K = 0。
    ・  INPUT -> CONV -> RELU -> FC
    ・  INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC。
        ここでは、すべてのPOOLレイヤー間に単一のCONVレイヤーがあることがわかる。
    ・  INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC
        ここでは、各POOLレイヤーの前に2つのCONVレイヤーが積み重ねられている。
        複数のスタック化されたCONVレイヤーは、
        破壊的プーリング操作の前に入力ボリュームのより複雑な機能を開発できるため、
        これは一般的にはより大きいネットワークとより深いネットワークにとっては良い考えである。

1つの大きな受容野CONV層への小さなフィルタCONVの積み重ねが好ましい。





