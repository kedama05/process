Convolutional Neural Networks
畳み込みニューラルネットワーク

畳み込みニューラルネットワークは、前の章の通常のニューラルネットワークと非常によく似ている。
学習可能な重みと偏りを持つニューロンで構成されている。

各ニューロンは、
いくつかの入力を受け取り、ドット積を実行し、任意に、それを非線形性で追跡する。

ネットワーク全体は、
一方の生の画像ピクセルから他方のクラススコアまで、
単一の微分可能なスコア関数を依然として表現している。

最後の（完全に接続された）層にはまだ損失関数（例えばSVM / Softmax）があり、
通常のニューラルネットワークを学習するために開発したすべてのヒント/トリックは依然として適用される。

ConvNetのアーキテクチャは、入力が画像であるという明示的な前提を定めているため、
アーキテクチャに特定のプロパティをエンコードすることができる。
→   フォワード機能の実装効率が向上し、ネットワーク内のパラメータ量が大幅に削減される。

各隠れ層は、
各ニューロンが前の層のすべてのニューロンに完全に接続されており、
単一の層のニューロンが完全に独立して機能し、
接続を共有していないニューロンのセットで構成されています。

最後に完全に接続されたレイヤは「出力レイヤ」と呼ばれ、分類設定ではクラススコアを表す。

レギュラーニューラルネットは、フルイメージには適していない。



Layers used to build ConvNets
ConvNetsの構築に使用されるレイヤー

単純なConvNetは一連のレイヤーであり、
ConvNetの各レイヤーは1つのボリュームのアクティベーションを異なるものに変換する。

畳み込みレイヤー、プールレイヤー、完全連結レイヤー（通常のニューラルネットワークとまったく同じ）、
3種類のレイヤーを使用してConvNetアーキテクチャーを構築する。

これらのレイヤーをスタックして、完全なConvNet アーキテクチャを形成する。



ConvNetアーキテクチャは、最も単純なケースでは、画像ボリュームを出力ボリュームに変換するレイヤーのリスト（例えば、クラススコアを保持する）
レイヤーにはいくつかの異なるタイプがある（たとえば、CONV / FC / RELU / POOLが最も人気がある）
各レイヤーは入力された3Dボリュームを受け取り、それを出力可能な3Dボリュームに変換することができる
各レイヤーにはパラメータがある場合とない場合がある（例えば、CONV / FC do、RELU / POOLはない）
各レイヤーには追加のハイパーパラメータがある場合とない場合がある（たとえば、CONV / FC / POOL do、RELUはない）


Convolutional Layer
畳み込みレイヤー

CONVレイヤのパラメータは、学習可能なフィルタのセットで構成されている。





プール層
ConvNetアーキテクチャでは、連続したConvレイヤの間に定期的にPoolingレイヤーを挿入するのが一般的。
    表現の空間サイズを漸進的に縮小して、
    ネットワークにおけるパラメータおよび計算の量を減らし、ひいては誇張を制御する。

プーリングレイヤーは、
    入力の各深度スライス上で独立して動作し、
    MAX操作を使用して空間的にサイズを変更する。

最も一般的な形式は、サイズが2x2のフィルタが適用されたプーリング層。
Poolingレイヤーにゼロパディングを使用するのは一般的ではない。


プールを取り除く
CONVレイヤーでより大きなストライドを使用することを推奨している。

プール・レイヤーを破棄することは、
バラエティオート・エンコーダー（VAE）または生成的敵対ネットワーク（GAN）などの
良い生成モデルを訓練する上で重要である。



ReLU
マイナスの数字が生じた時に、それを0に変換する。
学習値が0付近で立ち往生したり無限に向かったりすることがなくなり、CNNの健全性が保たれる。


正規化レイヤー



全結合層
    全結合層も、出力（投票のリスト）と入力（値のリスト）は全体的に類似しているため、
    その他の層と同じように積み重ねることができる。
    実際に複数の全結合層が一緒に積み重ねられることは少なくない。




ConvNetアーキテクチャ

    畳み込みネットワークは、
    CONV、POOL（別途明記されていない限りMaxプールとみなされます）と
    FC（完全接続の場合）との3種類のレイヤーで構成されている。

    RELU起動関数を要素的な非線形性を適用するレイヤーとして明示的に記述する。

レイヤパターン
    ConvNetアーキテクチャの最も一般的な形式
        いくつかのCONV-RELUレイヤーを積み重ね、POOLレイヤーに従い、
        画像が空間的に小さなサイズにマージされるまでこのパターンを繰り返す。

    ある時点では、完全に接続されたレイヤーに移行するのが一般的。
    最後に完全に接続されたレイヤーは、クラススコアなどの出力を保持する。

    つまり、最も一般的なConvNetアーキテクチャは、次のパターンに従う。
        INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC

        *: 反復, POOL?: オプションのプール層, 
        N >= 0 (and usually N <= 3), M >= 0, K >= 0

    ConvNetの一般的なアーキテクチャー
    ・  INPUT -> FC線形分類器を実装しています。ここでN = M = K = 0。
    ・  INPUT -> CONV -> RELU -> FC
    ・  INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC。
        ここでは、すべてのPOOLレイヤー間に単一のCONVレイヤーがあることがわかる。
    ・  INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC
        ここでは、各POOLレイヤーの前に2つのCONVレイヤーが積み重ねられている。
        複数のスタック化されたCONVレイヤーは、
        破壊的プーリング操作の前に入力ボリュームのより複雑な機能を開発できるため、
        これは一般的にはより大きいネットワークとより深いネットワークにとっては良い考えである。

1つの大きな受容野CONV層への小さなフィルタCONVの積み重ねが好ましい。



ケーススタディ

畳み込みネットワークの分野には、名前を持ついくつかのアーキテクチャがある。

    LeNet: Convolutional Networksの最初の成功したアプリケーションは、1990年にY​​ann LeCunによって開発された。
        これらのうち、最もよく知られているものは、郵便番号、数字などを読み取るために使用されたLeNetアーキテクチャ。
    
    AlexNet: コンピュータビジョンのコンボルーションネットワークを普及させた最初の仕事は、
        Alex Krizhevsky、Ilya Sutskever、Geoff Hintonによって開発されたAlexNetであった。
        AlexNetは2012年にImageNet ILSVRCチャレンジに提出され、
        第2次点（第5位の誤差は26％の誤差で第2位と比較して16％の誤差）を大幅に上回った。
        ネットワークはLeNetと非常に似たアーキテクチャをしていたが、
        より深く、より大きく、畳み込みレイヤーを積み重ねていた。
        （これまでは常にCONVレイヤーをPOOLレイヤーの直後に置いていた）
    
    ZF Net: ILSVRC 2013の受賞者はMatthew ZeilerとRob Fergusの畳み込みネットワーク。
        ZFNet（Zeiler＆Fergus Netの略）として知られるようになった。
        特に、中間畳み込みレイヤーのサイズを拡張し、第1レイヤーのストライドとフィルターサイズを小さくすることで、
        アーキテクチャーのハイパーパラメーターを調整することで、AlexNetが改善された。
    
    GoogLeNet: ILSVRC 2014受賞者は、Szegedyらの畳み込みネットワーク。Googleから。
        その主な貢献は、ネットワーク内のパラメータ数を劇的に削減したインセプションモジュールの開発（4M、AlexNetと比較して60M）。
        さらに、この論文では、ConvNetの最上位にあるFully Connectedレイヤの代わりにAverage Poolingを使用しているため、
        大した問題ではない大量のパラメータが削除されている。
        また、GoogLeNetにはいくつかのフォローアップバージョンがあり、
        最近ではInception-v4となっている。
    
    VGGNet: ILSVRC 2014で準優勝したのはKaren SimonyanとAndrew Zissermanのネットワークで、
        VGGNetとして知られるようになった。
        その主な貢献は、ネットワークの深さが良好なパフォーマンスを得るための重要な要素であることを示すことにあった。
        彼らの最終的なベストネットワークには16のCONV / FCレイヤが含まれており、
        魅力的なことに、最初から最後まで3x3コンボリューションと2x2プーリングしか実行しない極めて均質なアーキテクチャを特長としている。
        彼らのあらかじめ訓練されたモデルCaffeのプラグアンドプレイで使用できる。

        VGGNetの欠点は、評価するのがより高価であり、
        さらに多くのメモリとパラメータ（140M）を使用することである。

        これらのパラメータのほとんどは、最初に完全に接続されたレイヤーにあり、
        パフォーマンスダウングレードなしでこれらのFCレイヤーを削除でき、
        必要なパラメーターの数を大幅に削減できることが判明した。
    
    ResNet: Kaiming Heらによって開発された残留ネットワーク ILSVRC 2015の優勝者。
        特別なスキップ接続とバッチ正規化を頻繁に使用している。
        アーキテクチャでは、ネットワークの最後に完全に接続されたレイヤーが欠落している。
        この読者は、Kaimingのプレゼンテーション（ビデオ、スライド）、
        およびTorchでこれらのネットワークを再現した最近の実験についても言及されている。
        ResNetsは現在、最先端の畳み込みニューラルネットワークモデルであり、
        実際にConvNetsを使用するためのデフォルトの選択肢である（2016年5月10日現在）。
        特に、元のアーキテクチャをKaiming Heら 深い残差ネットワークにおけるアイデンティティマッピング（2016年3月公開）で最新の開発を見れる


VGGNetの詳細

VGGNet全体は、
ストライド1とパッド1で3x3コンボリューションを実行するCONVレイヤーと、
ストライド2（パディングなし）で最大2x2プールを実行するPOOLレイヤーで構成されている。

処理の各ステップで表現のサイズを書き出し、表現のサイズと総重量の数の両方を追跡することができる。

INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728
CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864
POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728
CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456
POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824
POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296
POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296
POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0
FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448
FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216
FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000

TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)
TOTAL params: 138M parameters


畳み込みネットワークでよく見られるように、
メモリの大部分（および計算時間も）は初期のCONVレイヤで使用され、
ほとんどのパラメータは最後のFCレイヤにある。


計算上の考慮事項

・  中間のボリュームサイズから：
    これは、ConvNetの各レイヤーでのアクティベーションの生の数であり、
    グラデーションも同じサイズ。

    通常、ほとんどのアクティベーションは、
    ConvNetの最初の層（つまり、最初のコンバ層）にある。

    これらはバックプロパゲーションに必要であるために保持されるが、
    テスト時にのみConvNetを実行する巧妙な実装では、
    現在のアクティベーションをすべてのレイヤーに格納し、
    下のレイヤーで以前のアクティベーションを破棄することによって、
    これを大幅に削減することができる。




・  パラメータサイズから：
    これらは、ネットワークパラメータ、バックプロパゲーション中のグラジエント、
    および最適化がモーメンタム、Adagrad、またはRMSPropを使用している場合のステップキャッシュを保持する数値である。

    したがって、パラメータベクトルのみを格納するメモリには、
    通常、少なくとも3倍程度の係数を掛けなければならない。



・  すべてのConvNetの実装では、イメージデータのバッチや、拡張バージョンなど、
    さまざまなメモリを維持する必要がある。


（アクティベーション、グラデーション、その他の場合の）合計値の概算値を取得したら、
その数値をGB単位のサイズに変換する必要がある。
値の数を4に掛けて、浮動小数点数が4バイト、倍精度の場合は8であるため、
生のバイト数を取得し、1024で複数回除算してKB単位のメモリ量を取得し、MB、そして最後にGB。
ネットワークが適合しない場合は、通常、メモリの大半がアクティベーションによって消費されるため、
バッチサイズを小さくするのが一般的なヒューリスティックである。