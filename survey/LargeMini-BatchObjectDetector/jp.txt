MegDet: A Large Mini-Batch Object Detector  [11 Apr 2]


Abstract

R-CNN、Fast / Faster R-CNNから
最近のマスクR-CNNおよびRetinaNetまでの深い学習の時代におけるオブジェクト検出の開発は、
主に新規ネットワーク、新しいフレームワーク、または損失設計から来ています。

しかしながら、
深いニューラルネットワークのトレーニングのための重要な要素であるミニバッチサイズは、
物体検出のために十分に研究されていない。

本稿では、大規模ミニバッチオブジェクト検出器（MegDet）を提案し、
最大256個のミニバッチサイズで最大128個のGPUを効果的に利用して
トレーニング時間を大幅に短縮できるようにする。


技術的には、
    ウォームアップ学習率ポリシーとCross-GPU Batch Normalizationをお勧めします。

これにより、
    大量のミニバッチ検出器をはるかに短い時間（例えば33時間から4時間）で
    うまく訓練することができます。

MegDetは、
    COCO2017チャレンジに対する私たちの提出（mmAP 52.5％）のバックボーンであり、
    ここで第1位の検出タスクを獲得しました。



1. Introduction

R-CNN、Fast / Faster R-CNNシリーズ、Mask R-CNNやRetinaNetのような
    最近のstate-of-theart検出器の精巧な作業から、
    →   CNNベースのオブジェクト検出には驚異的な進歩 がありました。

COCO [25]データセットを例にとると、
    Fast R-CNNの19.7 AP（10）からRetinaNetの39.1 AP（24）にわずか2年間で性能が向上しました。

この改善は主に、
    より良いバックボーンネットワーク[16]、新しい検出フレームワーク[31]、
    新規損失設計[24]、改善されたプーリング方法[5]、[14]などによるものである。


最近のCNNベースの画像分類の傾向は、
非常に大きな最小バッチサイズを使用してトレーニングを大幅にスピードアップします。

例えば、
    ResNet-50のトレーニングは、ミニバッチサイズ8,192または16,000を使用して
    1時間または13分で実行することができます
    （精度はほとんどまたはまったく犠牲になりません）。

契約では、
    ミニバッチサイズは、物体検出文献において非常に小さい（例えば、2~16）ままである。

したがって、本論文では、
    物体検出におけるミニバッチサイズの問題を研究し、
    大きなミニバッチサイズ物体検出器の訓練に成功するための技術的解決策を提示する。

小さなミニバッチサイズで何が問題になりますか？

    物体検出器R-CNNシリーズに由来する、2枚の画像のみを含むミニバッチは、
        高速R-CNNおよびR-FCNのような一般的な検出器に広く採用されている。

    RetinaNetおよびMask R-CNNのような最先端の検出器では、
        ミニバッチサイズは16に増加するが、
        現在の画像分類で使用されるミニバッチサイズ（例えば、256）と比較して
        →   依然として非常に小さい。


小さなミニバッチサイズにはいくつかの潜在的な欠点

    まず、
        トレーニング時間は非常に長いです。

        たとえば、ResNet152のCOCOトレーニングは、
        Titian XP GPUが8台あるマシンでミニバスサイズ16を使用して3日間かかります。

    第2に、
        小さなミニバッチサイズでのトレーニングは、
        バッチ正規化[20]（BN）のための正確な統計値を提供することができない。

        良好なバッチ正規化統計を得るために、
        ImageNet分類ネットワークのミニバッチサイズは、通常、
        現在のオブジェクト検出器設定で使用されるミニバッチサイズよりも
        かなり大きい256に設定される。


    最後に述べるが決して軽んずべきでないものであるが、
        小さなミニバッチ内の肯定的および否定的な訓練の例が不均衡になりやすく、
        最終的な精度が損なわれる可能性があります。

    図2は、不均衡なプラスとマイナスの提案を含むいくつかの例を示しています。

    表1は、異なるミニバッチサイズの2つの検出器の統計を
    COCOデータセットの異なる訓練エポックで比較しています。


単純に最小バッチサイズを増やすという課題は何ですか？

    画像分類の問題と同様に、我々が直面している主なジレンマは、
        「等価学習率ルール」[13,21]に従って、大規模な最小バッチサイズは通常、
        精度を維持するために大きな学習率を必要とすることである。

    しかし、オブジェクト検出における学習率が高いと、
    コンバージェンスが失敗する可能性が非常に高くなります。
    コンバージェンスを確実にするために学習率を小さくすると、
    結果が劣ることがよくあります。


上記のジレンマに取り組むために、以下のような解決策を提案する。

    まず、
        線形スケーリングルールの新しい説明を提示し、
        学習率を漸増させるための「ウォームアップ」学習率政策[13]を借りる。

        これにより、
            トレーニングのコンバージェンスが保証されます。

    次に、 
        精度とコンバージェンスの問題に対処するために、
        より良いBN統計のためにCross-GPU Batch Normalization（CGBN）を紹介します。
        CGBNは精度を向上させるだけでなく、トレーニングをより安定させる。

        これは、
            急速に増加する計算能力を業界から安全に享受できるため、重要です。

    私たちのMegDet（バックボーンとしてのResNet-50）は、
    128 GPUで4時間でCOCOトレーニングを完了することができ、さらに高い精度を達成します。

    対照的に、
    小型のミニバッチカウンターパートは33時間かかり、精度は低くなります。

つまり、図1に示すように、
    イノベーションサイクルをほぼ一桁上げてパフォーマンスを向上させることができます。

MegDetをベースに、COCO 2017 Detection Challengeの第1位を確保しました。


3. Approach

このセクションでは、
    Large Mini-Batch Detector（MegDet）を紹介し、
    より短い時間でトレーニングを終了し、より高い精度を達成します

3.1. Problems with Small Mini-Batch Size
3.1。小さなミニバッチサイズの問題

初期のCNNベースの検出器は、
    Faster-RCNNとR-FCNの2つのような非常に小さなミニバッチサイズを使用します。
RetinaNetやMask R-CNNのような最先端の検出器でも、
    バッチサイズは16に設定されています。

小さなミニバッチサイズでトレーニングするときは、いくつかの問題があります。
    
    まず、
        小さなミニバッチサイズをトレーニングに利用する場合は、
        より長いトレーニング時間をかけなければなりません。

        図1に示すように、
            16のミニバッチサイズに基づくResNet-50検出器のトレーニングには
            30時間以上かかります。

        元のミニバッチサイズ2の場合、
            トレーニング時間は1週間以上になる可能性があります。

    第二に、
        検出器の訓練では、
            バッチ標準化の統計値を修正し、
            小さなミニバッチサイズはBN層の再トレーニングには適用されないため、
            事前計算値をImageNetデータセットで使用します。

        COCOとImageNetの2つのデータセットが大きく異なるため、
        これは最適以下のトレードオフです。

    最後に、
        少なくとも正と負のサンプルの比率が非常に不均衡になる可能性があります。

        表1では、正と負の訓練例の割合の統計を示します。

        ミニバッチサイズが小さいことは、特に最初のステージで、
        より不均衡なトレーニングの例につながることがわかります。
            この不均衡は、全体の検出性能に影響を及ぼす可能性があります。

冒頭で説明したように、
    ミニバッチサイズを単純に大きくすることは、
    収束と正確さとの間のトレードオフに対処する必要があります。

この問題に対処するために、まず大きなミニバッチの学習率ポリシーについて説明します。


3.2. Learning Rate for Large Mini-Batch
3.2。 ラージミニバッチの学習率

学習率ポリシーは、SGDアルゴリズムと強く関連しています。
したがって、
    まず、
        物体検出ネットワークの損失構造を見直し、
            Nが最小バッチサイズであり、l(x,w)がタスク固有損失であり、l(w)が正則化損失である
                formula(1)
            を検討する。

        より高速なRCNN [31]フレームワークおよびその変形[6,23,14]では、
            l(xi,w)はRPN予測損失、RPNバウンディングボックス回帰損失、予測損失、
            および境界ボックス回帰損失で構成されます。

ミニバッチSGDの定義によれば、
    訓練システムは重みwに関して勾配を計算し、各繰り返し後に勾配を更新する必要がある。

    N^←k・Nのようなミニバッチのサイズが変化するとき、
        学習率rもトレーニングの効率を維持するために適応されるべきであると考えられる。

    以前の研究[21,13,39]は、
        新しい学習率をr^←k・rに変える線形スケーリング規則を使用している。

    大きなミニバッチN^の1つのステップは、
        小さなミニバッチNのk個の累積ステップの有効性と一致しなければならないため、
        学習率rにも同じ比率kを掛けて損失のスケーリングファクタに対抗する必要がある

    これは、SGDアップデートのグラディエント等価仮定[13]に基づいています。
        この経験則は、画像分類において十分に検証されており、
        物体検出には依然として適用可能であることがわかっている。

        しかし、解釈はより弱く、より良い仮定のために異なっている。
        However, the interpretation is different for a weaker and better assumption.

    画像分類では、
        すべての画像が1つの注釈しか持たず、
        l（x、w）は単純な形式のクロスエントロピーである。

    物体検出に関しては、
        全ての画像が異なるボックス注釈の数を有し、
        その結果、画像間でground-truth分布が異なる。

    2つのタスクの違いを考慮すると、
        異なるミニバッチサイズ間の勾配等価性の仮定は、
        オブジェクト検出において保持されにくい可能性がある。

    そこで、以下の分散分析に基づく別の説明を紹介する。

Variance Equivalence.
分散等価。

    勾配等価仮定とは異なり、勾配の分散はkステップの間は同じままであると仮定する。

    ミニバッチサイズNが与えられると、
        各サンプル∇l（xi、w）のiδに従う勾配であれば、
        l（x、w）上の勾配の分散は次のようになる。
            formula(2)
    
        同様に、大きなミニバッチN^ = k・Nに対して、次の式を得ることができます。
            formula(3)
        
        重み更新に等価性を期待する代わりに、ここでは、
            小さなミニバッチNのk個の累積ステップに等しい
            大きなミニバッチNの1つの更新の分散を維持したいと考えています。
            これを達成するために、我々は以下を持っている：
                formula(4)

    式（2）および（3）の中で、
        r^ = k・rならば、上記の等式(4)が成り立ち、
        これはr^について同じ線形スケーリング・ルールを与える。

    最終的なスケーリングの規則は同じですが、
        大規模なミニバッチの訓練では、勾配に関する同等の統計を維持できると考えているため、
        式（4）の分散等価性の仮定は弱いです。

ここでの分散分析が、
    幅広いアプリケーションでの学習率の深い理解に光を当てることを願っています。


Warmup Strategy.
ウォーミングアップ戦略。

    [13]で議論したように、
        線形スケーリングのルールは、重みの変化が劇的であるため、
        トレーニングの初期段階では適用できないことがあります。

        この実用的な問題に対処するために、[13]でLinear Gradual Warmupを借りる。

    つまり、
        rのように初めに学習率を十分に小さく設定します。

    次に、
        反復ごとにr^まで一定の速度で学習率を上げる。

ウォームアップ戦略はコンバージェンスに役立ちます。

しかし、
    実験で実証したように、より大きなミニバッチサイズ、
    例えば128または256では十分ではない。

次に、
    大規模なミニバッチ訓練の主役であるCross-GPU Batch Normalizationを紹介します。


4. Experiments


5. Concluding Remark
私たちは大型ミニバッチサイズ検出器を発表しました。
これは、はるかに短い時間でより良い精度を達成しました。

これは、私たちの研究サイクルが大幅に加速されたために顕著です。

その結果、私たちはCOCO2017検出の課題の1位を獲得しました。
詳細は付録にあります。


Appendix

私たちはMegDetに基づいて、
OHEM、atural convolution、より強固な基盤モデル、
大規模カーネル、セグメンテーション監視、多様なネットワーク構造、
コンテキストモジュール、ROIAlign、
COCO 2017 Object Detection Challengeの
マルチスケールトレーニングとテストなどの技術を統合します。

検証セットで50.5 mmAP、テストデベロッパーで50.6 mmAPを得ました。

4つの検出器のアンサンブルは最終的に52.5を達成した。

表4は、COCO2017チャレンジのリーダーボードからのエントリーをまとめたものです。

図5は、いくつかの例示的な結果を示す。