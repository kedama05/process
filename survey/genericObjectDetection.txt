3.1 Region Based (Two Stage Framework)

p6
Faster RCNN
Different from Fast RCNN, the features used for regression in RPN have the same size.
RPN shares CONV features with Fast RCNN, thus enabling highly efficient region proposal computation.

RPN is a kind of FullyConvolutional Network (FCN) 
Faster RCNN is thus a purely CNN based framework without using handcrafted features.

VGG16 model: 5fps
画像ごとに300の提案を使用してPASCAL VOC 2007で最先端のオブジェクト検出


The initial Faster RCNN in [175] contains several alternating training steps.
This was then simplified by one step joint training in [176].
[175]の最初のより高速のRCNNには、いくつかの交互のトレーニングステップが含まれています。
これは、[176]の1ステップの共同訓練によって単純化された。


高速RCNNの開発と並行して、CNNベースの検出器の領域提案生成の役割を研究し、
CNNがFCレイヤではなくCONVでの正確なオブジェクト検出に十分な幾何情報を含むことを発見した 。

CNNに専ら依存する統合された、より単純で高速なオブジェクト検出器を構築し、
選択的探索などの領域提案生成方法を除去する可能性を証明した。



RFCN(Region based Fully Convolutional Network)
Faster RCNNはFast RCNNよりも一桁高速ですが、
領域ごとのサブネットワークがまだRoI（画像あたり数百RoI）ごとに適用される必要があった。
    ↓
RFCN検出器: 画像全体にわたって共有されているほぼすべての計算と巧みに畳み込む（隠されたFCレイヤーなし）

Faster RCNN
    RoIプーリング層の後の計算は共有できません。

自然なアイデア
    共有できない計算量を最小限に抑えること
    ↓
DaiはすべてのCONVレイヤを共有RoIサブネットワークを構築することを提案
    RoI作物は予測の前にCONVフィーチャの最後のレイヤから取得する。

    この単純な設計は、より深いCONVレイヤーがカテゴリ意味に対してより敏感で、
    翻訳には敏感ではないと推定される一方で、
    オブジェクト検出には翻訳差異を考慮したローカライゼーション表現が必要であると推定される。

この観察に基づいて、
    特殊なCONVレイヤーのバンクをFCN出力として使用
    より標準的なRoIプーリングとは異なるポジティブセンシティブRoIプーリングレイヤーを追加して、
    一連のポジションセンシティブスコアマップを作成

    ResNet101 [79]を備えたRFCNが頻繁により速いランニングタイムで
    Faster RCNNと同等の精度を達成できることを示した



Mask RCNN

RCNN
    概念的な単純性、効率性、柔軟性を念頭に置いて、
    より高速なRCNNを拡張することによって、
    ピクセル単位のオブジェクトインスタンスセグメンテーションに取り組むことを提案

Mask RCNN
    同一の第1ステージ（RPN）を有する同じ2ステージパイプラインを採用する。

    第2段階では、クラスおよびボックスオフセットを予測することと並行して、
    Mask RCNNは、各RoIに対してバイナリマスクを出力するブランチを追加する。

        新しいブランチ: CNN機能マップの上にある完全畳み込みネットワーク（FCN）

    RoIAlign層: 元のRoIプーリング（RoIPool）層によって引き起こされた不整合を回避するために
                ピクセルレベルの空間的対応を維持するために提案された

    Mask RCNNは、
    バックボーンネットワークResNeXt101-FPN [223,130]を使用すると、
    COCOオブジェクトインスタンスのセグメンテーションと
    バウンディングボックスオブジェクトの検出で最高の結果を達成

    訓練するのが簡単、一般化が容易、
    高速のRCNNにわずかなオーバーヘッドしか加えず、5FPSで結合



Light Head RCNN
    RFCNの検出速度を向上させるために、LiはLight Head RCNNを提案し、
    検出ネットワークの頭部を可能な限り軽くしてRoI領域の計算を減らした。

    大きなカーネルの分離可能な畳み込みを適用
    小さなチャンネル番号と安価なRCNNサブネットワークを持つ薄いフィーチャマップを作成
    速度と精度の優れたトレードオフにつながった




3.2 Unified Pipeline (One Stage Pipeline)

3.1節で紹介した重要な取り組みにより、より迅速かつ正確な検出器が得られ、
一般的なベンチマークデータセットの現在の主要な結果はすべて、
Faster RCNN [175]に基づいています。

その進歩にもかかわらず、領域ベースのアプローチは、
ストレージおよび計算能力が制限された
モバイル/ウェアラブルデバイスにとって 計算上高価 になる可能性がある。
    ↓
複雑な領域ベースのパイプラインの個々のコンポーネントを最適化するのではなく、
統一された検出戦略 を開発し始めた。


統一されたパイプライン
    単一のフィードフォワードCNNネットワークを用いて、
    領域の提案生成またはポスト分類を伴わないモノリシックな設定で、
    クラス確率およびバウンディングボックスオフセットを
    フル画像から直接予測するアーキテクチャを広く参照する。

    このアプローチは、
        単一のネットワークで すべての計算をカプセル化 して、
        領域提案の生成 および
        その後のピクセルまたはフィーチャの リサンプリング段階を完全に排除 する
        →   シンプルで洗練 されています

    検出パイプライン全体は単一のネットワーク
    →   検出性能に直接 エンドツーエンドで最適化 することができます


DetectorNet
    object bounding boxマスクに対する回帰問題を定式化します。

    AlexNet [109]を使用し、最終的なsoftmax分類層を回帰層で置き換えます。

    画像ウィンドウが与えられれば、
        1つのネットワークを使用して 粗いグリッド上の前景ピクセルを予測 するとともに、
        オブジェクトの上、下、左および右の半分を予測するための 4つの追加のネットワークを使用 する

    グループ化プロセスは、
        予測されたマスクを検出されたバウンディングボックスに変換する。


    オブジェクトタイプとマスクタイプごとにネットワークを訓練する必要があります。
    それは複数のクラスに拡張されません。

多くの作物を取り込んで、各作物ごとに各部分ごとに複数のネットワークを実行する必要がある。




OverFeat
    fully convolutional deep networksに基づく最初の現代の1段オブジェクト検出器の1つ

    ILSVRC2013ローカリゼーション競争に勝利し、最も成功したオブジェクト検出フレームワークの1つ

    （最終的な分類/回帰層を除いて）
    畳み込み層のみからなるCNNネットワークを通る単一の順方向パスを介して、
    マルチスケールスライディングウィンドウ方式 で オブジェクト検出 を実行する。
    →   自然に重複領域間で計算を共有します。


    特徴ベクトルの グリッドを生成 する。
        各グリッドは入力画像内のコンテキストビューの位置がわずかに異なり、
        オブジェクトの存在を予測することができる


    オブジェクトが識別されると、
    →   同じフィーチャを使用して、単一の境界ボックス回帰子を予測


    ネットワークを介して元画像の最大6つの拡大尺度を渡し、それらを繰り返して集約
    →   総合的な性能を向上させるためにマルチスケール機能を活用

    同じ期間に提案されたRCNNよりもはるかに高速ですが、
    その段階で完全な畳み込みネットワークを育成することは困難
    →   精度が大幅に低下します。

速度の利点
    完全畳み込みネットワークを用いて
    重なり合うウィンドウ間の畳み込み計算を共有することから得られる。



YOLO (You Only Look Once)
    画像ピクセルから空間的に分離されたバウンディングボックスおよび
    関連するクラス確率までの回帰問題として物体検出を統一する検出器

    領域プロポーザルの生成ステージが完全に削除されるため、
    →   候補領域の小さなセットを使用して検出を直接予測します。

    領域ベースのアプローチとは異なり、
        地方の地物の特徴に基づいて検出を予測する高速RCNNは、
        画像全体の特徴を世界的に使用します。

YOLO: 画像をS×Sグリッドに分割する。
    各グリッドは、
        Cクラスの確率、Bバウンディングボックスの位置、
        およびそれらのボックスの信頼スコアを予測します。

        これらの予測は、S×S×（5B + C）テンソルとして符号化される。

    地域提案提案の生成手順を完全に廃止することで、
    →   YOLOは設計上高速であり、45FPSでリアルタイムで実行され、
         高速バージョン、すなわち155FPSで動作するFastYOLO [174]が実行される。


YOLOは予測を行うときに画像全体を見るので、
→   オブジェクトクラスに関するコンテキスト情報を暗黙的にエンコードし、
    →   バックグラウンドでの誤検出を予測する可能性は低くなります。


YOLOは、
    バウンディングボックスの位置、縮尺、アスペクト比の大まかな分割の結果として、
    →   より多くのローカライゼーションエラーを発生させます。

[174]で議論されているように、
    YOLOはグリッド分割がかなり粗いため、
    いくつかのオブジェクト、特に小さなオブジェクトを
    ローカライズできない可能性があります。

YOLOが
    ILSVRC検出の課題など、かなり多くのオブジェクトを含むデータセットで
    良好なパフォーマンスにどの程度まで変換できるかは不明です。




YOLOv2 and YOLO9000
    YOLOの改良バージョン

    カスタムGoogLeNet [200]ネットワークがより単純なDarkNet19で置き換えられ、
    バッチ標準化[78] 完全に接続されたレイヤーを削除し、
    kmeansとマルチスケールトレーニングで学んだ良いアンカーボックスを使用します。

    PASCAL VOCやMS COCOのような標準的な検出タスクで最先端の技術を達成。

    RedmonとFarhadi [173]はYolO9000を導入
        YOLO9000は、
            複数のソースからのデータを結合するためにWordNetを使用して
            ImageNetとCOCOを同時に訓練する共同最適化方法を提案することにより、
            リアルタイムで9000種類以上のオブジェクトカテゴリを検出できます。


SSD (Single Shot Detector)
    YOLO [174]よりも速く、Faster RCNN [175]を含む
    最先端の領域ベースの検出器と競合する精度を有する

    あまりにも多くの検出精度を犠牲にすることなくリアルタイム速度を維持するために提案

    高速なRCNN [175]、YOLO [174]、multiscale CONV features[75]のRPNのアイデアを
    効果的に組み合わせて、
    →   高い検出品質を維持しながら高速な検出速度を実現

    YOLOと同様に、SSDは、
        これらのボックス内にオブジェクトクラスインスタンスが存在するかどうかについて
        固定数のバウンディングボックスとスコアを予測し、最後にNMSステップを実行
        →   最終検出を生成します。

    SSDのCNNネットワークは完全に畳み込みであり、
        初期層は基本ネットワークと呼ばれる
        VGG（分類層の前に切り捨てられている）などの標準アーキテクチャに基づく

    サイズが徐々に減少するいくつかの補助CONVレイヤがベースネットワークの最後に追加

    低解像度の最後のレイヤの情報は、空間的には粗すぎて、正確な定位ができないことがある
    →   SSDは、小さな物体を検出するためにより高い解像度の浅い層を使用

    サイズの異なるオブジェクトの場合、
        複数のCONV feature mapsで操作することにより、
        →   複数のスケールにわたって検出を実行
        
        複数のCONV feature mapsのそれぞれは、
           適切なサイズのbounding boxesのcategory scoresとbox offsetsを予測

300×300の入力の場合、
    SSDはNvidia Titan Xの59 FPSでのVOC2007テストで74.3％mAPを達成します。



4 Fundamental SubProblems
    フィーチャリプレゼンテーション、地域提案、コンテキスト情報マイニング、
    トレーニング戦略など、重要なサブ問題について


4.1 DCNN based Object Representation
4.1 DCNNベースのオブジェクト表現

いずれの検出器の主要構成要素の1つとして、
良好な特徴表現 が物体検出において第一の重要性を有する

過去には、
    SIFTやHOGなどのローカル記述子を設計し、
    その記述子をより高いレベルの表現にグループ化して抽象化する
    Bag of WordsやFisher Vectorなどのアプローチを
    探索することに多大な努力が払われてきたが、
    これらの特徴表現方法は、慎重なengineeringとかなりの領域の専門知識を必要とする



対照的に、
    複数の処理層で構成されたディープ学習法（特にディープCNNやDCNN）は、
    生の画像から直接複数の抽象レベルを持つ強力な特徴表現を学習することができる。

    学習手続きが伝統的なフィーチャ工学で必要とされる特定のドメイン知識と
    複雑な手続きの依存を減らすにつれて、
    →   フィーチャ表現の負担はより良いネットワークアーキテクチャの設計に移された。

主なフレームワークは、検出精度と速度を絶えず促進

CNN表現は重要な役割を果たし、
    検出器のエンジンであるのはCNNアーキテクチャであることは一般に認められている。

結果として、
    検出精度の最近の改善の大部分は、
    新規ネットワークの開発に関する研究によって達成されている。

したがって、
    まず、汎用オブジェクト検出で使用される一般的なCNNアーキテクチャを見直し、
    不変特徴の開発など、オブジェクトの特徴表現を改善するための努力を見直し、
    ↓
    物体のスケール、姿勢、視点、部品の変形の幾何学的変化に適応し、
    広範囲のスケールにわたって物体検出を改善するためのマルチスケール解析を実行する。


4.1.1 Popular CNN Architectures
4.1.1一般的なCNNアーキテクチャ

代表的なフレームワーク
    AlexNet、ZFNet、VGGNet、GoogLeNet、Inceptionシリーズ、ResNet、DenseNet、SENet

    これらは表2にまとめられており、
    オブジェクト認識におけるネットワークの改善は図9からわかる。
    最近のCNN進歩のさらなる見直しは、[71]に見出すことができる。


簡単に説明すると、
    CNNは
        階層構造を有し、畳み込み、非線形性、プールなどのような多数の層から構成される。

        より細かい層からより粗い層まで、
        画像は繰り返しフィルタリングされた畳み込みを受け、
        各層でこれらのフィルタの受容野（支持領域）が増加する。

    例えば、先駆的なAlexNet
        5つの畳み込みレイヤーと2つのFully Connected（FC）レイヤー
            第1レイヤーにはサイズ11×11×3の96個のフィルターを含む

            第1のCNN層: 低レベルのフィーチャ(エッジなど)を抽出
            中間層: 低レベルの特徴の組み合わせのような複雑さが増す特徴を抽出
            後の畳み込みレイヤ: オブジェクトをConclusions

    アーキテクチャの進化の傾向は、ネットワークConclusions
        AlexNetは8層、VGGNetは16層で構成され、Conclusions
        最近ではResNetとDenseNetの両方が100層のマークを上回り、
        特に、VGGNetとGoogLeNetで
        深層化が深層ネットワークの表現力を向上させることが示されました。

    興味深いことに、表2から分かるように、
    AlexNet、OverFeat、ZFNet、およびVGGNetなどのネットワークは、
    パラメータの大部分がFC層から来ているため、
    →   数層しかないにもかかわらず膨大な数のパラメータを有する。

        ↓
    Inception、ResNet、DenseNetなどの新しいネットワークは、
    ネットワーク深度が非常に大きいものの、
    FCレイヤーの使用を避けることで、パラメータが大幅に減少する。


    慎重に設計されたトポロジでInceptionモジュールを使用することで、
    GoogLeNetのパラメータが大幅に削減される。

    ResNetは、
        ILSVRC 2015分類作業に勝利し、
        何百ものレイヤーを持つ非常に深いネットワークを学習するためのスキップ接続の有効性を実証しました。


    ResNet [79]に触発されてInceptionResNets [202]は、
        インセプションネットワークをショートカット接続と組み合わせ、
        ショートカット接続がインセプションネットワークのトレーニングを
        大幅に加速すると主張している。

    ResNetsを拡張するHuangは
        密集したブロックから構築されたDenseNetsを提案しました。
        
        密なブロックがフィードフォワードの方法で各レイヤーをすべてのレイヤーに接続し、
        パラメータ効率、暗黙の深い監督、および機能再利用などの魅力的な利点につながります。


    最近、Huは、
        ILSVRC 2017の分類作業に勝利した、
        畳み込み特徴チャンネル間の相互依存性を明示的にモデル化することによって、
        チャネルごとの特徴応答を適応的に再較正することにより、
        既存の深いアーキテクチャと組み合わせて、
        最小の追加の計算コストで性能を向上させることができる
        Squeeze and Excitationブロックと呼ばれるアーキテクチャユニットを提案した。
        
CNNアーキテクチャに関する研究は依然として活発であり、
    Dilated Residual Networks [230]、Xception [35]、
    DetNet [127]、Dual Path Networks（DPN）[31]など、
    数多くのバックボーンネットワークが未だに出現している。

p11
The training of a CNN requires a large labelled dataset with
sufficient label and intraclass diversity. Unlike image classification,
detection requires localizing (possibly many) objects from an
image. It has been shown [161] that pretraining the deep model





p16
4.3 Detection Proposal Methods
オブジェクトは、画像内の任意の位置に配置できます。

手作りのフィーチャ記述子（SIFT、HOG、LBPなど）の盛り上がりの間、
BoA（Bag of Words）とDPMはスライディングウィンドウ技術を使用しました。

しかし、ウィンドウの数は大きく、画像内のピクセル数とともに増加し、
    複数のスケールおよびアスペクト比で検索する必要性は、
    検索スペースをさらに大幅に増加させる。

したがって、より洗練された分類器を適用するのは計算上高価すぎる。

2011年前後に、
    研究者は検出の提案を用いて計算の扱いやすさと
    高い検出品質の間の緊張を緩和することを提案した[3] [210]。

        [2]で提案されたオブジェクト性の考え方に由来するオブジェクト提案は、
        オブジェクトを含む可能性が高いイメージ内の候補領域の集合です。

        検出プロポーザルは、通常、
            検出器によって評価される必要がある領域の数を制限することによって
            計算の複雑さを低減するために、前処理ステップとして使用される。

        したがって、優れた検出提案には次の特性が必要です。

            1.  高いリコール。わずかな提案でしか達成できない。
            2.  提案はオブジェクトを可能な限り正確に一致させます。
            3.  高い効率。

選択的探索[210、209]によって与えられる検出提案に基づく物体検出の成功は、
大きな関心を集めている[21,7,3,33,254,50,105,144]。

オブジェクト提案はオブジェクト検出以外のアプリケーションを持っているため、
→   オブジェクト提案アルゴリズムの包括的なレビューはこのホワイトペーパーの範囲外です[6,72,252]。

興味深い読者を、
    多くの古典的なオブジェクト提案アルゴリズムの深い分析と
    検出パフォーマンスへの影響を提供する最近の調査[86,23]に紹介します。


ここでは、
    DCNN、出力クラスに依存しないプロポーザル、
    および一般的なオブジェクト検出に関連するオブジェクト提案方法を検討します。

2014年には、
    オブジェクト提案[210、209]とDCNN機能[109]の統合により、
    汎用オブジェクト検出におけるマイルストーンRCNN [65]が導かれました。

それ以来、検出提案アルゴリズムは、
    2014年以降の検出提案を使用して以来、
    PASCAL VOC、ILSVRC、およびMS COCOのオブジェクト検出の課題が
    すべて勝ったという事実によって証明される標準的な前処理ステップになりました。

従来の低レベル手がかり（例えば、色、テクスチャ、エッジ及びグラジエント）に基づくオブジェクト提案手法の中で、Selective Search [209]、MCG [7]及びEdgeBoxes [254]がより一般的である。


領域が急速に進むにつれて、
    検出器から独立した外部モジュールとして採用された従来のオブジェクト提案手法
    [例えば、選択的探索[209]および[254]は、
    検出パイプラインのボトルネックとなった[86]。

DCNNを使用した
    新しい提案クラスのオブジェクト提案アルゴリズム[52,175,111,61,167,224]が注目されている。

最近のDCNNベースのオブジェクト提案方法は、一般に、
    バウンディングボックスベースおよび
    オブジェクトセグメントベースの2つのカテゴリに分類され、
    代表的な方法が表5にまとめられている。


Bounding Box Proposal Methodsは、
    図14に示すRenらのRPC法[175]によって最もよく例示される。

RPNは、
    最後の共有されたCONVレイヤのフィーチャマップ上に小さなネットワークを
    スライドさせることによってオブジェクトの提案を予測します（図14参照）。

    各スライディングウインドウ位置では、
        k個のアンカーボックスを使用することによってk個の提案を同時に予測し、
        各アンカーボックス4は、画像内のある位置にセンタリングされ、
        特定のスケールおよびアスペクト比に関連付けられる。

Renは、
    畳み込みレイヤを共有することにより、
    RPNとFaster RCNNを単一のネットワークに統合することを提案しました。

このような設計は、
    大幅なスピードアップと最初のエンドツーエンド検出パイプライン、
    Faster RCNN [175]をもたらしました。

RPNは、表3および表4から分かるように、
        多くの最先端の物体検出器による提案方法として広く選択されている。


LuはMultiBox [52,199]とRPN [175]のような先験的なアンカーのセットを固定する代わりに、
    オブジェクトを含む可能性のある部分領域に焦点を合わせるように
    計算資源を適応的に導くことができる再帰的探索戦略を用いて
    アンカー位置を生成することを提案した。

画像全体から始めて、
    検索プロセス中に訪れたすべての領域がアンカーとして機能します。

検索処理中に検出されたアンカー領域では、
    スカラーズームインジケーターを使用して領域をさらに分割するかどうかを決定し、
    オブジェクト性スコアを持つバウンディングボックスのセットを、
    近接ネットワークおよびズームネットワーク（AZNet）と呼ばれる
    深いネットワークで計算します。

AZNetは、
    ブランチを追加してRPNを拡張し、
    スカラー・ズーム・インジケータを既存のブランチと並列に計算します。

多層畳み込み特徴[103,61,224,122]を利用して
    オブジェクト提案を生成しようとするさらなる研究がある。

RPNと並行して、Ghodratiは、
    複数の畳み込み機能のカスケードを使用して
        オブジェクト提案を生成するDeepProposalを提案し、

    逆カスケードを構築して、
        最も有望なオブジェクトの場所を選択し、
        粗いから細かい方法でボックスを洗練しました。


改良されたRPNの変形であるHyperNet [103]は、
    多層畳み込み特徴を集約し、エンドツーエンドの共同訓練戦略によって提案を生成し、
    物体を検出する際にそれらを共有するハイパーフィーチャを設計する。

最後に、
最近注目された作品には、
    EdgeBoxによって生成された提案を再学習するための軽量CNNを提案したDeepboxや、
    より高速なRCNNスタイルの2ステージ検出器で
    RPNを置き換えるためにオブジェクト提案を効率的に予測するための
    バウンディングボックスコーナー推定を導入するDeNetなどがあります。

ヤン氏は、
    カスケード戦略を用いたCRAFT [224]を提案した。
        最初にRPNネットワークを訓練してオブジェクト提案を生成し、
        次にそれらを使って別のバイナリーFast RCNNネットワークを訓練し、
        バックグラウンドとオブジェクトをさらに区別した。

Liは、
    低レベルの詳細と高レベルのセマンティクスの両方を統合するために、
        ネットワークの異なる深度で複数の畳み込み特徴マップを用いて
        オブジェクト提案を予測する一般的に使用される考えを利用して
        RPNを改善するZIPを提案した。

    ZIPで使用されるバックボーンネットワークは、
        convおよびdeconv構造からインスピレーションを受けた
        「ズームアウトアンドイン」ネットワークです[138]。



オブジェクト・セグメント・プロポーザル・メソッドは、
    オブジェクトに対応する可能性が高い
    セグメント・プロポーザルを生成することを目的としています。

    セグメント・プロポーザルは、
        バウンディングボックスの提案よりも有益であり、
        オブジェクトインスタンスセグメンテーションに向けてさらに進んでいます。

    先駆的な研究は、
        Pinheiroによって提案されたDeepMaskであり、
        セグメントの提案は、深いネットワークを持つ生の画像データから直接学習される。

        DeepMaskは、
            ネットワークを2つのブランチに分割して、
            クラスにとらわれないマスクと関連するオブジェクト性スコアを予測します。

        OverFeat [183]の効率的なスライディングウィンドウ予測戦略と同様に、
            訓練されたDeepMaskネットワークは、
            推論の間にスライディングウィンドウ方式で画像
            （および再スケールされたバージョン）に適用されます。

    より最近では、
        Pinheiroは、図11（b1）および（b2）に示すアーキテクチャと同様の
        洗練モジュールを使用してDeepMaskアーキテクチャを拡張することで
        SharpMaskを提案し、
        フィードフォワードネットワークをトップダウン洗練プロセスで補強しました。


        SharpMaskは、
            初期フィーチャからの空間的に豊富な情報を
            後のレイヤでエンコードされた強力な意味情報と効率的に統合して、
            高忠実度のオブジェクトマスクを生成することができる。


    Semantic segmentation [138]とDeepMask [167]のために
    完全畳み込みネットワーク（FCN）に動機付けられたDaiは
        インスタンスセグメントの提案を生成するInstanceFCN [38]を提案した。

        DeepMaskと同様に、
            InstanceFCNネットワークは2つのブランチに分割され、
            2つのブランチが完全に畳み込みされ、
            1つのブランチ
                インスタンスに影響を受けるスコアマップの小さなセットを生成し、
            その後に生成される別のブランチ
                インスタンスを出力するアセンブルモジュールと、オブジェクト指数を予測する

    Huは、
        深いネットワークでマルチスケールの畳み込み特徴を利用するために、
        SSD [136]と同様にoneshot方式でインスタンスセグメントの提案を効率的に生成する
        FastMask [89]を提案した。

    マルチスケール畳み込み特徴マップから密に抽出されたスライディングウィンドウを、
    スケール許容性の注意力ヘッドモジュールに入力して、
        セグメンテーションマスクおよびオブジェクト性スコアを予測した。

    FastMaskは800x600の解像度の画像で13FPSで動作すると主張されており、
        平均リコールではわずかなトレードオフがあります。

    Qiaoは、
        ShaleMask [168]のような以前のオブジェクト提案手法をスケール予測段階を
        明示的に追加することによって拡張することをScaleNetに提案しました。

つまり、ScaleNetは入力画像のオブジェクトスケールの分布を推定し、
    SharpMaskはScaleNetで予測されたスケールで入力画像を検索し、
        インスタンスセグメントの提案を出力します。

Qiaoは、彼らの方法が、
    スーパーマーケットデータセット上の以前の技術水準を大幅に上回ることを示した。



4.4 Other Special Issues

より良い、より堅牢なDCNN特徴表現を得ることを目指して、
データ増大トリックが一般に使用される[22,64,65]。
    トレーニング時、テスト時、またはその両方で使用できます。

    拡大は、
        クラスの追加のサンプルを生成するために、
        クロッピング、反転、回転、スケーリング、平行移動など、
        基礎となるカテゴリを変更しないまま変換することによって
        イメージを混乱させることを意味します。

データ拡張は、深い特徴表現の認識性能に影響する可能性がある。
    ↓
それにもかかわらず、明らかな限界があります。

訓練と推論の計算の複雑さが大幅に増加し、
実際のアプリケーションでの使用が制限されています。

広範囲のスケール変動のもとで物体を検出し、
特に非常に小さな物体を検出すること は、
→   重要な課題の1つとして際立っている。

画像の解像度は
→   検出精度に大きな影響を与えることが示されている[96,136]

したがって、高分解能入力は小さな物体が検出される可能性を大きくするため、
→   これらのデータ拡大トリックの中では
     スケーリング（特に高解像度入力）が主に使用されます[96]。

        96. Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A., Fischer I.,
            Wojna Z., Song Y., Guadarrama S., Murphy K. (2017) Speed/accuracy
            trade-offs for modern convolutional object detectors 11, 19, 23


    最近、シンは、
        表6に要約されているように、スケール不変性問題を説明するために、
        高度で効率的なデータ議論方法SNIP [192]およびSNIPER [193]を提案した。

        小規模および大規模の物体を
        それぞれより小さいおよびより大きなスケールで検出することが
        困難であるという直感的な理解によって動機づけられ、
        
        Singh氏は、
            SNIPという名前の新しいトレーニングスキームを発表した。
            これは、トレーニング中のスケール変動を減らすことができます。
 
        SNIPER [193]は
            効率的なマルチスケール訓練のために提案されたアプローチである。

            全体画像ピラミッドを処理するのではなく、適切なスケールで
            地面真理オブジェクトの周りのコンテキスト領域を処理するだけです。

    ShrivastavaとLinは、
        極端な前景 - 背景クラスの不均衡の問題[131]に対処するアプローチを模索した。

    Wangは、
        オブジェクト検出器が認識することが困難な
        咬合および変形を伴う例を生成するために、
        adversarial networkを訓練することを提案した。

    非最大抑制のためのより良い方法の開発に焦点を当てたいくつかの研究がある[16、87、207]。

p19
5 Datasets and Performance Evaluation
5.1 Datasets

データセットは、オブジェクト認識研究の歴史を通じて重要な役割を果たした。

    競合するアルゴリズムの性能を測定して比較する共通の根拠としてだけでなく、
    ますます複雑かつ困難な問題に向けてフィールドを押し進めることで、
    現場でのかなりの進歩にとって最も重要な要素の1つとなっています。


インターネット上の多数の画像への現在のアクセスは、
    ますます豊かで豊富な対象物を捕捉するために、
    増加する数の画像およびカテゴリの包括的なデータセットを構築することを可能にする。

数百万の画像を含む 大規模データセットの登場 により、
    画期的な画期的な進歩が遂行され、
    物体認識において前例のない性能が実現しました。

Caltech101以降、代表的なデータセットには、
    Caltech256、Scenes15、PASCAL VOC（2007）、
    Tiny Images、CIFAR10、SUN、ImageNet、
    Places、MS COCO、Open Images
    などがあります。

Caltech101やCaltech256などの以前のデータセットは、
    クラス内のバリエーションがないために批判されました。

その結果、SUN [221]は、
    様々なシーンカテゴリを描写する画像を見つけることによって収集され、
    その画像の多くは、
    シーン認識および物体検出をサポートすることができる
        シーン注釈およびオブジェクト注釈を有する。

Tiny Images [204]は、
    前例のないスケールでデータセットを作成し、
    すべてのオブジェクトカテゴリとシーンを包括的にカバーしていますが、
    信頼性の高いラベルを持つ2つのベンチマークがTiny Imagesから得られました。
    (CIFAR10とCIFAR100)
    しかし、注釈は手動では検証されず、多数のエラーが含まれていました。
    

PASCAL VOC [53,54]は、
    分類とオブジェクト検出のための
    一連のベンチマークデータセットの作成と
    メンテナンスに多年にわたる取り組みを行っており、
    年次大会の形式で認識アルゴリズムの標準化評価の先例を作り出しています。

    図15に示すように、
        2005年にはわずか4つのカテゴリから始まり、
        日常生活で共通する20のカテゴリに増加しました。

ImageNet [44]には、
    ILSVRC [44,179]チャレンジのバックボーンである
    14,000,000以上の画像と20,000以上のカテゴリが含まれています。
    →   物体認識研究が新たな高さに押し上げられました。

    データセット内のオブジェクトが大きくて中心に置かれ、
    実際のシナリオの非典型的なデータセットになる傾向があると批判されています。


この問題を解決し、より豊かな画像理解に研究を進めるという目標のもと、
    研究者はMS COCOデータベースを作成した[129]。

MS COCOの画像は、
    実際の生活に近い自然なコンテキストで
    共通のオブジェクトを含む複雑な日常シーンであり、
    オブジェクトは完全なセグメント化されたインスタンスを使用してラベル付けされ、
    より正確な検出器評価を提供する。


Placesデータベース[245]には、
    シーンセマンティックカテゴリのラベルが付いた1000万のシーンイメージが含まれており、
    データが欲しい深い学習アルゴリズムが視覚パターンの人間レベルの認識に達する機会を提供します。


より最近では、Open Images [106]は、
    画像レベルのラベルとオブジェクト境界ボックスで注釈付けされた約9百万の画像のデータセットです。


ジェネリックオブジェクト検出には3つの有名なchallengesがあります。
    PASCAL VOC [53,54]、ILSVRC [179]、およびMS COCO [129]。

各チャレンジは、2つのコンポーネントから成ります
    （i）   一般公開されている画像のデータセットと
            グランドトゥルーアノテーションと標準化された評価ソフトウェア

    （ⅱ）   年次大会およびそれに対応するワークショップ。


検出challengesのための訓練、検証および試験データセット5における
画像およびオブジェクトインスタンスの数に関する統計は表8に示されている。

PASCAL VOCチャレンジでは、
    2009年以来、データは新しい画像で補強された前年度の画像から成り、
    毎年画像の数を増やすことができます。
    さらに重要なことは、テスト結果を前年度の画像と比較できることです。

ILSVRC [179]は、
    PASCAL VOCの標準化された訓練および検出アルゴリズムの評価の目標を、
    オブジェクトクラスおよび画像の数のオーダー以上に拡大する。

ILSVRCオブジェクト検出チャレンジは、
    2013年から現在まで毎年実行されています。

COCOオブジェクトの検出の課題は、
    一般的なオブジェクト検出の最先端技術を進歩させるために設計されており、
    2015年から現在まで毎年実行されています。

    バウンディングボックス出力またはオブジェクトインスタンスセグメンテーション出力の
    いずれかを使用する2つのオブジェクト検出タスクがあります。

    ILSVRCよりもオブジェクト・カテゴリーは少なくなりますが、
    カテゴリーごとに多くのインスタンスがあります。
        ILSVRC（COLSで80 vs ILSVRCオブジェクト検出で200）
        カテゴリごとのインスタンス（平均約11000、ILSVRCオブジェクト検出で約2600）

    ILSVRCでは現在利用できないオブジェクトセグメンテーションアノテーションも含まれています。

COCOにはいくつかの新しい課題がありました。
    （1）小さなパーツの高いパーセンテージ（例えば、画像領域[192]の1％未満）
        を含む広範囲のスケールのオブジェクトを含む。

    （2）オブジェクトが象徴的でなく、混乱や重い閉塞の中にある。

    （3）評価メトリック（表9を参照）は、
        より正確なオブジェクトのローカリゼーションを促進する。

COCOは、
    一般的なオブジェクト検出のための最も広く使用されるデータセットになっており、
    トレーニング、検証、テストのデータセット統計は表8にまとめられています。


2017年以降、
    テストセットにはDev-Challengeスプリットのみがあり、
    Test-Devスプリットはデフォルトのテストデータであり、
    結果は一般的に公正な比較を可能にするためにTest-Devで報告されます。

2018年には、
    PASCAL VOC、ImageNet、COCOの伝統に引き続き、
    オープンイメージオブジェクト検出チャレンジが導入されましたが、
    これまでにないスケールで開催されました。

以前の課題よりも幅広いオブジェクトクラスを提供し、次の2つのタスクがあります。
    500個の異なるクラスの bounding box object 検出
    特定の関係におけるオブジェクトのペアを検出する視覚的関係検出。


5.2 Evaluation Criteria

検出アルゴリズムのパフォーマンスを評価する基準には、
    検出速度（フレーム/秒、FPS）、精度、リコールの3つがあります。


最も一般的に使用される指標は、
    精度とリコールに由来する平均精度（AP）です。
    APは、通常、カテゴリ固有の方法で評価され、
    すなわち、各オブジェクトカテゴリに対して別々に計算される。

ジェネリックオブジェクト検出では、通常、
    検出器は多数のオブジェクトカテゴリを検出するという観点からテストされる。

すべてのオブジェクトカテゴリにわたるパフォーマンスを比較するために、
    すべてのオブジェクトカテゴリにわたって平均化された平均AP（m AP）が、
    パフォーマンスの最終尺度として採用される。

テスト画像Iに適用される検出器の標準出力は、
    jによってインデックスされる予測検出{（bj、cj、pj）} jである。


所与の検出（b、c、p）（表記の簡略化のためにjを省略）は、
    予測カテゴリラベルcおよびその信頼水準pを有する予測位置
    （すなわち、バウンディングボックス、BB）bを示す。

予測される検出（b、c、p）は、以下の場合に真陽性（TP）とみなされる。
    ・  予測クラスラベルcは、地上真理ラベルcgと同じです。
    ・  予測されたBB bと地上の真理値1 bgとの間の
        オーバーラップ率IOU（交点上の交点）は、所定の閾値ε以上である。 
        ここで、領域（b∩bg）は、予測された真のBBと、真の真のBBとの交点と、
        領域（b∪bg）の和集合を表す。 
        典型的なεの値は0.5です。

それ以外の場合は、偽陽性（FP）とみなされます。

信頼レベルpは、通常、
    あるクラスの閾値βと比較され、
    予測クラスラベルcが受け入れられるかどうかが判定される。


APは、
    PrecisionとRecallに基づいて、各オブジェクトクラスに対して個別に計算されます。

所与のオブジェクトクラスcおよびテスト画像Iiに対して、
    式（0）は、確信度pijによって
    降順にランク付けされた検出器によって返された検出を示すものとする。
    
与えられたオブジェクトクラスcについて、
    式（01）を画像Iiのグラウンドトゥルースボックスとする。

各検出（bij、pij）は、TPまたはFPのいずれかであり、
    図16のアルゴリズムを介して決定することができる。


TPおよびFP検出に基づいて、
    精度P（β）およびリコールR（β）を信頼閾値βの関数として計算することができるので、
    信頼閾値を変更することによって異なるペア（P、R）を得ることができ、
    
    原理的には、
        正確さを平均値（AP）[53,179]が見い出されるリコールの関数、
        すなわちP（R）と見なすことを可能にする。


表9は、PASCAL、ILSVRCおよびMS COCOオブジェクト検出の課題で使用される主なメトリックをまとめたもの


p22
5.3 Performance
    PASCAL VOC [53,54]、ImageNet [179]、COCO [129]などの標準ベンチマークの導入により、
    検出器の精度を比較することが容易になりました。

    第3章と第4章の議論からわかるように、
    以下のような基本的/文脈上の点で異なる可能性があるため、
    精度、スピード、メモリだけで検出器を客観的に比較することは困難です。

    ・  RCNN [65]、高速RCNN [64]、高速RCNN [175]、RFCN [40]、マスクRCNN [80]、
        YOLO [174]およびSSD [136]などのメタ検出フレームワーク。

    ・  表2に示すVGG [191]、Inception [200,99,201]、ResNet [79]、
        ResNeXt [223]、Xception [35]、DetNet [127]などのバックボーンネットワーク。

    ・  多層的な特徴の組み合わせ[130,190,58]、変形可能な畳み込みネットワーク[41]、
        変形可能なRoIプーリング[160,41]、重い頭部[177,164]、軽い頭部[128]などの革新。

    ・  ImageNet [179]、COCO [129]、Places [245]、JFT [82]、
        Open Images [106]などのデータセットによる事前訓練

    ・  異なる検出提案手法と異なる数のオブジェクト提案。

    ・  マルチマイクロ、水平フリッピング、マルチスケール画像、
        新規マルチスケールトレーニング戦略[192、193]など、マスクの締め付け、
        およびモデルのアンサンブルなどのデータ補強の "トリック"のトレーニング/テスト。

    最近提案されたあらゆる検出器を比較することは実用的ではないかもしれないが、
    代表的かつ公に利用可能な検出器を共通プラットフォームに統合し、
    それらを統一的に比較することは非常に貴重である。



    バックボーンネットワーク、画像解像度、ボックス提案数などを変えることで、
    3つの主要な検出器（Faster RCNN、RFCNおよびSSD）ファミリの精度と速度との間の
    トレードオフのHuangの研究[96]を除いて、
    この点に関しては非常に限られた研究がなされている。


    表3、表4、表5、表6および表10からわかるように、
    広く使用されている3つの標準ベンチマークについて、
    多くの方法の中で最もよく報告されているパフォーマンスを要約しました。

    これらの方法の結果は、
    上に列挙された1つまたは複数の態様がそれぞれ異なるにもかかわらず、
    同じテストベンチマークで報告されました。


図1および図17は、
    PASCAL VOC、ILSVRCおよびMSCOCOの課題の最良の検出結果をまとめた
    最先端技術の概要を示しています。

より多くの結果が検出挑戦のウェブサイト[98、148、163]で見つかることができます。



検出の3つの最も重要な要素
    バックボーンネットワーク
    検出フレームワーク設計
    大規模データセットの可用性
    

より良い精度を達成するのに役立つ
    複数のモデルのアンサンブル
    コンテキストフィーチャの組み込み
    およびデータ拡大はすべて

5年未満では、AlexNet [109]が提案されて以来、
    図9に示すように1000クラスを持つImageNet分類[179]のTop5エラーは16％から2％に低下


しかし、
    COCO [129]上で最良の検出器[164]（80クラスを検出するように訓練されただけ）のmAPは、
    0.5 IoUにおいてさえ73％に達し、
    オブジェクト検出が画像分類よりもはるかに困難であるかを明確に示している。

    164.    Peng C., Xiao T., Li Z., Jiang Y., Zhang X., Jia K., Yu G., Sun J. (2018)
            MegDet: A large minibatch object detector. In: CVPR 20, 23

最先端の検出器によって達成される精度レベルは、
    汎用の実用的なアプリケーションの要件を満足するものではなく、
    将来の改善の余地がまだ残っています。



6 Conclusions

一般的な物体検出は、
    コンピュータビジョンにおいて重要かつ困難な問題であり、かなりの注目を集めている。

深い学習技術の顕著な発展のおかげで、オブジェクト検出の分野は劇的に進化しました。

この論文では、
    一般的なオブジェクト検出のための深い学習に関する包括的な調査として、
    最近の成果を強調し、検出における役割、要約された既存のデータセット
    および評価基準に従ってメソッドの構造分類を提供し、
    →   最も代表的なメソッドのパフォーマンスについて議論しました。


    過去数年間に達成された驚異的な成功にもかかわらず、
    （例えば、ILSVRC2013の23％からILSVRC2017の73％まで有意に改善する検出精度）
    特にオープンワールドの学習という点で、
    最先端と人間レベルのパフォーマンスの間には大きな隔たりがあります。


    次の8つの領域に焦点を当てていると、まだ多くの研究が行われています。

    
(1) Open World Learning:
（1）オープン・ワールド・ラーニング：
    究極の目標は、
        人間の視覚システムと競合するすべてのオープンワールドシーンで、
        すべてのオブジェクトカテゴリ（数千またはそれ以上のオブジェクトクラス[43]）の
        インスタンスを正確かつ効率的に認識およびローカライズできる
        オブジェクト検出システムを開発することです。

    最近のオブジェクト検出アルゴリズムは、
        限られたデータセットで学習され、
        データセットに含まれるオブジェクトカテゴリを認識してローカライズしますが、
        原則としてデータセット外の他のオブジェクトカテゴリには盲目的になります。
    理想的には、
        強力な検出システムは新規のオブジェクトカテゴリを認識できなければならない。

    現在の検出データセット[53,179,129]は、
        数十から数百までのカテゴリーしか含んでおらず、
        人間が認識できるものよりもはるかに小さい。

    この目標を達成するためには、
        CNNの最先端技術が十分に訓練するためには大量のデータを必要とするため、
        →   汎用オブジェクト検出のためにはるかに多くのカテゴリを持つ
             新しい大規模ラベル付きデータセットを開発する必要がある

    そのような膨大な量のデータ、
        特に物体検出のためのボックスラベルを収集することは、
        →   特に数十万のカテゴリにとって 非常に高価 である。



(2) Better and More Efficient Detection Frameworks:
（2）より効果的な検出フレームワーク：

    ジェネリックオブジェクト検出における大きな成功の要因の1つは、
        領域ベースおよび1状態検出器の両方で、より優れた検出フレームワークの開発
            領域ベース（RCNN [65]、Fast RCNN [64]、FasterRCNN [175]、マスクRCNN [80]）
            1状態検出器（YOLO [174]、SSD [136]）

        
    領域ベースの検出器は
        最高精度を有するが、
        組み込みシステムまたはリアルタイムシステムには計算量が多すぎる。

    1ステージ検出器は、
        より高速で簡単な可能性を秘めていますが、
        まだ領域ベースの検出器の精度には達していません。


1つの可能な制限は、
    最先端のオブジェクト検出器が基礎となる
    最初に画像分類のために最適化されたバックボーンネットワークに大きく依存した、
    
    DSOD検出器のように、
        1つの潜在的な戦略が物体検出器をゼロから学習することである
    
    という分類と検出の違いによる学習バイアスを引き起こすことである。


(3) Compact and Efficient Deep CNN Features:
（3）コンパクトで効率的な深いCNNの特徴：

generic object detectionにおける相当な進歩のもう一つの重要な要因は、
    数層(AlexNet)から数百層(ResNet, DenseNet)に顕著に深く増加した
    強力な深層CNNの開発であった。

これらのネットワークには
    数百万〜数億のパラメータがあり、
    訓練のために大量のデータとパワーが必要なGPUを必要とし、
    アプリケーションをリアルタイム/組み込みアプリケーションに限定します。

これに対応して、以下に関心が集まっている
    コンパクトで軽量なネットワーク
    ネットワークの圧縮と加速
    ネットワークの解釈と理解の設計
    


(4) Robust Object Representations:
（4）ロバストなオブジェクト表現：

    オブジェクト認識の問題を困難にする重要な要素の1つは、
        実世界のイメージの大きな変化
            視点や照明の変化
            オブジェクトのスケール
            オブジェクトのポーズ
            オブジェクトのパーツの変形
            背景のクラッタ
            オクルージョン
            外観の変化
            イメージのぼやけ
            イメージの解像度
            ノイズ
            カメラの限界および歪み
        を含む、様々な要因に依存する。


深いネットワークの進歩にもかかわらず、
    実際のアプリケーションのユーザビリティを著しく制限する
    これらの多くのバリエーションに対する堅牢性の欠如によって
    依然として制限されている。


(5) Context Reasoning:
（5）コンテキスト推論：

実際のオブジェクトは、通常、
    他のオブジェクトや環境と共存します。

文脈情報（オブジェクト関係、グローバルシーン統計）は、
    特にオブジェクトが小さくて隠れているか、画質が悪い場合に、
    
    →   オブジェクトの検出と認識に役立つことが認識されている[155]。


深い学習[143,152,171,47,59]に先行して広範な作業が行われていたが、
深い学習期以来、文脈情報を利用する進歩はごく限られていた[29,62,90]。

コンテキスト情報を効率的かつ効果的に取り込む方法は、
    人間が自然のシーンで関心のある対象物に
    どのように素早く関心を向けるかによって理想的には探求されています。


object instance segmentationは、
    個々のインスタンスの正確な境界を必要とする多くの潜在的なアプリケーションで
    重要な役割を果たす可能性があるため、
ピクセルレベルの object instance segmentation に取り組むことが、
    →   より豊かでより詳細なイメージコンテンツの理解に向かう傾向にあります。


(7) Weakly Supervised or Unsupervised Learning:
（7）弱監督または非監督の学習：

現在の最先端技術の検出器は、
    オブジェクトバウンディングボックスまたはセグメンテーションマスクを用いて
    ラベル付きデータから学んだ完全に監視されたモデルを使用するが、
    そのような完全に監視された学習は
    境界ボックス注釈の仮定が問題になる可能性がある場合、
    特にオブジェクトカテゴリの数が多い場合に重大な制限がある。

十分に監視された学習は、
    完全にラベル付けされたトレーニングデータがない場合にはスケーラブルではないため、
    弱い教師付きまたは非教師付きの検出で
    CNNの能力をどのように活用できるかを研究することは重要である[15,45,187]。


(8) 3D Object Detection:
（8）3Dオブジェクト検出：

深度カメラの進歩により、
    RGB-D画像または3D点群の形での奥行き情報の取得が可能になった。

深さモダリティは、
    物体の検出と認識を助けるために使用することができますが、
    この方向での作業は限られていますが、
    高品質のCADモデルの大きなコレクションを活用することで利益を得ることができる[219]。


generic object detectionの研究分野はまだ完全ではありません。
    過去5年間で大規模なアルゴリズムのブレークスルーがあったことから、
    今後5年間の機会について楽観的な姿勢を維持しています。
