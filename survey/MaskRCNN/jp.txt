Mask R-CNN

Abstract
    我々は、
        オブジェクト・インスタンス・セグメンテーションのための、
        概念的に単純で柔軟で一般的なフレームワークを提示する。

    我々のアプローチは、
        画像内のオブジェクトを効率的に検出すると同時に、
        各インスタンスに対して高品質のセグメンテーションマスクを生成する。

    Mask R-CNNと呼ばれるこの方法は、
        バウンディングボックス認識のために既存のブランチと並行して
        オブジェクトマスクを予測するためのブランチを追加することによって
        高速R-CNNを拡張する。

    マスクR-CNNはトレーニングが簡単で、
        5fpsで動作する高速R-CNNにわずかなオーバーヘッドしか追加しません。

    さらに、マスクR-CNNは、
        例えば、同じフレームワーク内の人間のポーズを推定することを可能にするなど、
        他のタスクに一般化することは容易である。

        インスタンスのセグメンテーション、バウンディングボックスオブジェクトの検出、
        人のキーポイントの検出など、
        COCOの一連の課題の3つすべてのトラックに最高の結果を示します。

        ベルとホイッスルがなければ、
            マスクR-CNNは、COCO 2016の挑戦受賞者を含む
            すべてのタスクの既存のすべてのシングルモデルエントリーよりも優れています。

        私たちは、
            シンプルで効果的なアプローチが確実なベースラインとして役立ち、
            インスタンスレベルの認識で将来の研究を容易にすることを願っています。

        コードは https://github.com/facebookresearch/Detectron から入手できます。

1. Introduction
    ビジョンコミュニティは、
        短時間でオブジェクト検出と
        セマンティックセグメンテーションの結果を迅速に改善しました。

    大部分において、これらの進歩は、
        オブジェクト検出およびセマンティックセグメンテーションのための
        Fast / Faster RCNN [12,36]および
        完全畳み込みネットワーク（FCN）[30]フレームワークなどの
        強力なベースラインシステムによってそれぞれ推進されてきました。

    これらの方法は、
        概念的に直感的で柔軟性と堅牢性を提供し、迅速な訓練と推論時間を提供します。

    この作業の目標は、
        インスタンスセグメンテーションのための比較的有効なフレームワークを開発すること

    インスタンスのセグメンテーションは、
        各インスタンスを正確にセグメント化しながら、
        イメージ内のすべてのオブジェクトを正確に検出する必要があるため、困難

    したがって、
        オブジェクト検出の古典的なコンピュータビジョンタスクの要素を結合します。
        目的は、
            個々のオブジェクトを分類し、
            境界ボックスを使用して各オブジェクトをローカライズすることと、
            区別することなく各ピクセルを固定したカテゴリのセットに分類する
            セマンティックセグメンテーションですオブジェクトインスタンス。

    したがって、
        オブジェクト検出とセマンティックセグメンテーションという
        古典的なコンピュータビジョンタスクの要素を組み合わせています。

            オブジェクト検出の目的
                個々のオブジェクトを分類し、
                境界ボックスを使用して各オブジェクトをローカライズすること

            セマンティックセグメンテーションの目標
                各ピクセルをオブジェクトインスタンスを
                区別することなく固定されたカテゴリのセットに分類すること

        これを考えると、
            良い結果を得るためには複雑な方法が必要になると考えられる

        しかし、
            驚くほどシンプルで柔軟性があり、高速なシステムが、
            最先端のインスタンスセグメンテーションの結果を上回ることを示しています。

    我々の方法は、Mask R-CNNと呼ばれ、
        分類およびバウンディングボックス回帰のための既存のブランチと並行して、
        各関心領域（RoI）上のセグメンテーションマスクを予測するための
        ブランチを追加することによってFaster R-CNN [36]を拡張する。（図1）

    マスクブランチは、
        各RoIに適用される小さなFCNであり、
        ピクセルトップセル方式でセグメンテーションマスクを予測する。

    マスクR-CNNは、
        柔軟なアーキテクチャ設計の幅広い範囲を容易にする
        Faster R-CNNフレームワークを考慮すると、実装とトレーニングが簡単です。

    さらに、
        マスクブランチはわずかな計算オーバーヘッドを追加するだけであり、
        高速システムと迅速な実験が可能です。
    
    原則的に、マスクR-CNNは高速R-CNNの直感的な拡張であるが、マスクブランチを適切に構築することは良好な結果にとって重要である。

    最も重要なことは、
        Faster RCNNは、ネットワーク入力と出力の間の
        ピクセル間アライメントのために設計されていないことです。

    これは、
        インスタンスに参加するための事実上のコアオペレーションであるRoIPool [18,12]が、
        フィーチャ抽出のための粗い空間量子化を
        どのように実行するかにおいて最も明白である。

    ミスアライメントを修正するために、
        正確な空間的位置を忠実に保存する、
        RoIAlignと呼ばれる単純な量子化フリー層を提案します。

    外見上のマイナーな変更にもかかわらず、RoIAlignは大きな影響を与えます。
        マスク精度を相対10％〜50％向上させ、
        より厳しいローカリゼーションメトリックの下で大きな利益を示します。

    第2に、
        マスクとクラスの予測を切り離すことが不可欠であることを発見しました。
            クラス間の競合なしに、各クラスのバイナリマスクを独立して予測し、
            ネットワークのRoI分類ブランチに依存してそのカテゴリを予測します。

    

対照的に、FCNは通常、ピクセルごとのマルチクラス分類を実行します。これは、セグメンテーションと分類を結合します。


2. Related Work

















p4
Network Architecture:

    我々のアプローチの一般性を示すために、
        複数のアーキテクチャを持つMask R-CNNをインスタンス化する。

        分かりやすくするために、次の点を区別します。

        （i）   画像全体にわたる特徴抽出に使用される畳み込み主鎖アーキテクチャ

        （ii）  各RoIに個別に適用されるバウンディングボックス認識（分類および回帰）
                およびマスク予測のためのネットワークヘッド。

    私たちは、
        ネットワークの奥行き特徴という命名法を用いて、
        バックボーンのアーキテクチャを表しています。

    我々は、
        深さ50または101層のResNet [19]およびResNeXt [45]ネットワークを評価する。

        ResNets [19]を使用したFaster R-CNNの最初の実装では、
            C4と呼ばれる第4ステージの最終畳み込みレイヤからフィーチャを抽出しました。

        ResNet-50を使用したこのバックボーンは、たとえばResNet-50-C4で表されます。

        これは[19、10、21、39]で使われている一般的な選択です。

    我々はまた、
        Linらによって最近提案されたもう1つのより効果的なバックボーンを探索する。

        [27]、Feature Pyramid Network（FPN）と呼ばれています。

        FPNは、
            横方向の接続を備えたトップダウンアーキテクチャを使用して、
            単一スケールの入力からネットワーク内のフィーチャピラミッドを構築します。

        FPNバックボーンを備えたより高速のR-CNNは、
            フィーチャピラミッドの異なるレベルから
            RoIフィーチャをそのスケールに従って抽出しますが、
            それ以外の方法はvanilla ResNetに似ています。

        ResNet-FPNバックボーンを使用してマスクRCNNによるフィーチャ抽出を行うと、
        精度とスピードの両方において優れた利得が得られます。

        FPNの詳細については、[27]を参照してください。


    ネットワークヘッドに関しては、
        完全畳み込みマスク予測ブランチを追加する前の研究で
        提示されたアーキテクチャに密接に従う。

    具体的には、
        ResNet [19]とFPN [27]の論文のFaster R-CNNボックスヘッドを拡張する
        詳細を図4に示す

        ResNet-C4バックボーン上のヘッドには、
            ResNetの5番目のステージ（すなわち、9層のres5 [19]）が含まれています。

        FPNの場合、
            バックボーンにはすでにres5が含まれているため、
            より少ないフィルタを使用するより効率的なヘッドが可能になります。

        私たちのマスクブランチは簡単な構造を持っています。

        より複雑なデザインはパフォーマンスを向上させる可能性がありますが、
        この作業の焦点ではありません。

3.1. Implementation Details
    既存のFast / Faster R-CNN研究[12,36,27]に続いて、ハイパーパラメータを設定した。

    これらの決定は、
        オリジナルの論文[12,36,27]でのオブジェクト検出のために行われましたが、
        私たちのインスタンスセグメンテーションシステムは
        それらに堅牢であることがわかりました。

    Training:
        Fast R-CNNの場合と同様に、RoIは正とみなされます
            それが少なくとも0.5のグラウンドトゥルーボックスを有するIoUを有し、
            そうでなければ否定的である。

            マスク損失Lmaskは、正のRoIにのみ定義されます。

            マスクターゲットは、
                RoIとそれに関連するグラウンドトゥルースマスクとの間の交差点です。
        
        画像中心の訓練を採用する[12]。
            画像の縮尺（短辺）が800ピクセルになるように画像のサイズが変更されます[27]。

            各ミニバッチにはGPUあたり2枚の画像があり、
            各画像にはN個のサンプリングされたRoIがあり、1：3の正と負の比率があります[12]

            NはC4バックボーンでは64であり（[12,36]）
            FPNでは512である（[27]と同様）。

            160k回の反復で8つのGPU（効果的な最小サイズは16）を学習し、
            1202回の反復で0.02の学習率が10減少します。

            0.0001の重量減と0.9の運動量を使用します。

        ResNeXt [45]では、
            開始学習率0.01で、
            GPUあたり1つのイメージと同じ繰り返し回数でトレーニングを行います。

        RPNアンカーは、[27]に続いて、5つのスケールと3つのアスペクト比に及ぶ。
            便利なアブレーションのために、
                RPNは個別に訓練され、指定されない限り、マスクR-CNNと機能を共有しません。

            この論文のすべてのエントリについて、
                RPNとMask R-CNNは同じバックボーンを持ち、共有可能です。

    Inference:
        テスト時には、
            C4のバックボーンでは[36]のように300、FPNでは[27]のように1000となります。

        我々は、
            これらの提案についてボックス予測ブランチを実行し、
            非最大抑制を行った[14]。

        次いで、
            マスクブランチは、最高スコア100の検出ボックスに適用される。
                トレーニングで使用されている並列計算とは異なりますが、
                推論の速度を上げ、精度を向上させます
                （より少数で正確なRoIを使用するため）
        
        マスクブランチは、
            RoIごとにK個のマスクを予測することができるが、k番目のマスクのみを使用する。
                kは、分類ブランチによる予測クラスである。

        次に、m×m浮動小数点マスク出力は、
            RoIサイズにサイズ変更され、0.5の閾値で2値化される。

        マスクR-CNNは、
            上位100個の検出ボックス上のマスクのみを計算するので、
                マスクR-CNNは、
                    その比較的高速なR-CNN相手に小さなオーバーヘッドを加える。
                    （例えば、典型的なモデルでは約20％）


4. Experiments: Instance Segmentation


    4.1. Main Results

    4.2. Ablation Experiments

    4.3. Bounding Box Detection Results

    4.4. Timing


5. Mask R-CNN for Human Pose Estimation

    Implementation Details:

    Main Results and Ablations:


Appendix A: Experiments on Cityscapes

Appendix B: Enhanced Results on COCO

Instance Segmentation and Object Detection
    我々は表8のマスクR-CNNのいくつかの強化された結果を報告する。

    全体的に、この改善は、
        マスクAP 5.1ポイント（36.7から41.8へ）
        およびボックスAP 7.7ポイント（39.6から47.3へ）を増加させる。

    各モデルの改善は、
        マスクAPとボックスAPの両方を一貫して増加させ、
        マスクR-CNNフレームワークの良好な一般化を示している。

    私たちは次の改善について詳しく述べます。

    これらの結果は、
        今後の更新とともに、https://github.com/facebookresearch/Detectron
        のリリースコードで再現することができ、
        将来の研究のためのより高いベースラインとして役立ちます。

    更新されたベースライン：
        異なるハイパーパラメータのセットで更新されたベースラインから始めます。

        トレーニングを180k回繰り返すと、学習率は120kと160kの繰り返しで10ずつ減ります。

        また、NMSのしきい値を0.5（デフォルト値の0.3から）に変更します。

        更新されたベースラインは、37.0のマスクAPと40.5のボックスAPを有する。

    エンドツーエンドトレーニング：
        以前のすべての結果は、
            ステージワイズトレーニング、すなわち、第1ステージとしてRPNをトレーニングし、
            第2ステージとしてマスクR-CNNをトレーニングしたトレーニングを使用した。

            RPNとマスクRCNNを共同して
            トレーニングするエンドツーエンド（'e2e '）トレーニングを評価します[37]。

        我々は[37]の 'approximate'バージョンを採用しています。
            このバージョンでは、勾配w.r.t RoI座標を無視してRoIAlignレイヤーの部分勾配のみを計算します。

        表8は、
            e2eトレーニングがマスクAPを0.6、ボックスAPを1.2だけ改善することを示す。

    ImageNet-5k事前トレーニング：
        [45]の後、私たちは、
            ImageNetの5kクラスのサブセット（標準1kクラスのサブセットとは対照的に）
            で事前トレーニングされたモデルを実験する。

        事前トレーニングデータのこの5倍の増加は、
            マスクおよびボックス1 APの両方を改善する。

        参考までに、
            [40]は~250倍の画像（300M）を使用し、
            ベースラインで2-3ボックスAPの改善を報告しました。
    

    列車時間の増強：
        列車時のスケール拡大は結果をさらに改善する。

        トレーニング中に[640,800]ピクセルからスケールをランダムにサンプリングし、
        反復回数を260k（200kと240kの反復で学習率を10減らす）にします。

        列車時間の増加は、マスクAPを0.6、ボックスAPを0.8だけ改善する。

    モデルアーキテクチャ：
        101層のResNeXtを152層の対応物[19]にアップグレードすることにより、
            0.5のマスクAPと0.6のボックスAPの増加が観察されます。

        これは、より深いモデルがCOCOの結果を改善できることを示しています。

    最近提案された非ローカル（NL）モデル[43]を用いて、
        40.3マスクAPと45.0ボックスAPを達成する。

        この結果はテスト時間の延長ではなく、
            このメソッドはテスト時にNvidia Tesla P100 GPUで3fpsで実行されます。

    テスト時間の増加：
        [400,1200]ピクセルのスケールを使用して評価されたモデル結果を
        ステップ100で結合し、水平フリップで結合します。

            これにより、41.8のマスクAPと47.3のボックスAPの単一モデルの結果が得られます。

    上記の結果は、COCO2017競技会への提出の基礎となりました
    （ここでは説明していないアンサンブルも使用しました）

        インスタンスセグメンテーションタスクの最初の3チームはすべて、
        MaskR-CNNフレームワークの拡張に基づいていると伝えられていました。



    
    
    Table 3.
        オブジェクト検出単一モデル結果（バウンディングボックスAP）、最先端のtest-dev。

            ResNet-101-FPNを使用したマスクR-CNNは、
                従来のすべての最先端モデルの基本変形よりも優れています
                （これらの実験ではマスク出力は無視されます）。

            RoCAlign（+1.1 APbb）、マルチタスクトレーニング（+0.9 APbb）、
            およびResNeXt-101（+1.6 APbb）を使用することにより、
                Mask R-CNNの利点が得られます。



